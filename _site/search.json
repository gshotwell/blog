[
  {
    "objectID": "posts/2019-12-30-why-i-use-r/index.html",
    "href": "posts/2019-12-30-why-i-use-r/index.html",
    "title": "Why I Use R",
    "section": "",
    "text": "1. Native data science structures\nIt’s relatively easy to do data science in R without any external libraries. You can read data from a csv into a data frame, plot and clean that data, and analyse it using built-in statistical models. This is possible because R was built to do statistics, and so it includes features like vectors and data frames, and lets you invert a matrix or fit a linear model. Over time Python has added all of these capabilities with numpy, pandas, and scikitlearn, but you usually require dependencies to do data science work.\nI’m generally in favour of using external libraries, but it’s nice to have the option of avoiding them in certain circumstances. Data structures in the base language tend to be more stable than those provided by external dependencies. For instance, I can run base R code from 2010 and be reasonably sure that the code will behave the same way today as it did a decade ago, because maintainers of the core language are very conservative in introducing breaking changes.\nThis is probably not true of R or Python code that relies on libraries like dplyr or pandas because both of those libraries prioritize feature improvement over stability. This isn’t meant as a criticism of either of those libraries, but is meant just to point out that it is beneficial to use a language that allows you to do meaningful data science work without importing external libraries.\nOne of the weird things that you probably won’t hear from the “R isn’t a real language” Pythonistas is that there’s this whole group of production Python engineers who don’t think that the Python scientific computing stack is appropriate for production. These people will make the same basic argument about pandas that pandas people make about R: it’s appropriate for research, but production code requires vanilla Python. Switching from R to Python often doesn’t significantly reduce deployment friction, because you still have to do some kind of microservice process in order to isolate the data science dependencies from the rest of the Python code base.\n\n\n2. Non-Standard Evaluation\nR includes a strange and wonderful type of metaprogramming called Non-standard evaluation, which allows you to access and manipulate the calling environment of a function. This lets you do things like use a variable name in a plot title, or evaluate a user-supplied expression in a different environment.\nNSE reminds me of a niche and dangerous power tool:\nit shouldn’t be the first thing you reach for, and it’s very dangerous if you don’t know what you’re doing, but it allows you to solve problems that would be otherwise unsolvable.\n\n\n{{% youtube \"ox93snKVuaM\" %}}\n\n\nThere are three main ways that I use NSE:\n\nSeparate user representations from programmatic representations\nThere’s often a tension between the most natural way for a user to represent a problem and the best way to organize that problem internally. Internally, it’s good if the inputs to a system are unambiguously specified so that it’s crystal clear what the system should do, and how the system should be organized. In contrast, the user of a function often doesn’t need or want to know about the implementation details and instead wants to provide the inputs in the way that requires them to learn the fewest number of new things. Without NSE, it’s very hard to solve this problem because what goes into a function is what the function has to use, but NSE lets you capture and modify the expressions the user sends into the function so you can translate them into another form. For example, R lets you specify models with a formula interface like this: lm(mtcars, mpg ~ cyl). This is a natural way for statisticians to specify statistical models because they’re usually familliar with the syntax, but without NSE there’s no way to make that function work as written because mpg and cyl are not objects in the calling environment. NSE allows lm to capture the mpg ~ cyl and evaluate them within the data environment. To accomplish the same thing in a standard evaluation model you’d need to do some sort of string manipulation, which is what you find in the Python version.\nNote that you give up a lot by using NSE, and in almost every context it’s the wrong tool. You might look at the R and Python formula interface and think that giving up referential transparency isn’t worth avoiding a few quotation marks, but there are lots of cases where it saves the day.\nA more complex example is dplyr, which puts a consistent user facing api in front of dataframe and database backends.\nThe programmatic representation varies greatly across the different backends. Using NSE, the package captures the expressions supplied by the user and translates them into programmatic representations that are understood by the various backends.\n\n\nMake code more concise\nRecently I had a whole bunch of functions that all included a structure like this:\n\nf1 <- function(df) {\n  if (nrow(df) == 0) {\n    return(\n      data.frame(join_col = NA)\n    )\n  }\n  ### Lots of time consuming code ....\n}\n\nWhat I wanted to do was create a new function, returnIfEmpty, which caused its calling function to return the default dataframe if it were passed a dataframe without any rows. I can do this with NSE like this:\n\nreturnIfEmpty <- function(df) {\n  if (nrow(df) == 0) {\n    default <- data.frame(join_col = NA)\n    assign(\"return_data\", default, envir = parent.frame()) # Modify calling environment\n    call <- rlang::expr(return(return_data)) # Capture expression\n    rlang::eval_bare(call, env = parent.frame()) # Evaluate expression in calling environment\n  }\n}\n\nf1 <- function(df) {\n  returnIfEmpty(df)\n  ### Lots of time consuming code ....\n}\n\nf1(data.frame())\n\n  join_col\n1       NA\n\n\nIn this example, returnIfEmpty is first creating an object in its parent environment, then building a call to return that object, and finally evaluating that call in the parent environment. This was a good way for me to avoid a lot of code repetition, which I don’t think I would’ve been able to do otherwise.\n\n\nLearn the user’s language\nOne of the great things about accessing the user’s environment is that there’s a wealth of information in that environment that is particularly meaningful to that user. In particular, if you can use the names that a user assigned to a variable in function output or error messages it makes your function much easier for them to understand.\nConsider this function:\n\nregularError <- function(df) {\n  if (!inherits(df, \"data.frame\")) {\n    warning(\"df must be a dataframe\")\n  }\n}\n\nmy_var <- 1:10\nregularError(my_var)\n\nWarning in regularError(my_var): df must be a dataframe\n\n\nThe user doesn’t know anything about the internal arguments of your function, so they probably don’t know what the df in the warning message is referring to. In order to understand the warning, they need to read the documentation, or worse, the source code. This leads to confusion, because you’re forcing the user to learn your language rather than telling them what’s wrong in their own terms. NSE allows us to make the error much friendlier:\n\nfancyError <- function(df) {\n  class <- class(df)\n  var_name <- as.character(substitute(df))\n  if (!inherits(df, \"data.frame\")) {\n    warning(glue::glue(\"'{var_name}' is of class '{class}' when it needs to be a dataframe\"))\n  }\n}\nfancyError(my_var)\n\nWarning in fancyError(my_var): 'my_var' is of class 'integer' when it needs to\nbe a dataframe\n\n\nThis communicates what’s going wrong in terms that are much more meaningful to the user, because they assigned the name “my_var” to the object in the first place.\nYou might think that it’s too much effort to write a friendly error message, but in my work I find that details like this help build delightful products that are easy to use. It’s worth taking the time to communicate problems in the user’s language, and NSE is the best way I know to learn that language.\n\n\n\n3. The glory of CRAN\nI started programming on a Saturday morning during law school. In hindsight this was a very important morning for me because in many ways it shaped the course of my career for the next ten years. I probably had about 20 minutes to become interested and excited about the project, since I had lots of homework to do and programming wasn’t something that was on any of my to-do lists at the time. The resource I started with was “R Twotorials,” which taught you how to use R in two minute lessons.\n\nblogdown::shortcode(\"youtube\", \"5DZkQjPyzjs\")\n\n{{% youtube \"5DZkQjPyzjs\" %}}\n\n\nR let me get up and running, installing packages, filtering data, and printing plots in under 20 minutes, which meant that I stayed interested in the language and eventually started using it professionally. I had actually started to learn Python at around the same time but just found it too difficult. I didn’t know how to open a terminal window, I didn’t want to spend any time on configuration, and I didn’t have any time to devote to setup. Python required me to spend more than 20 minutes on setup, and R didn’t, so I picked R.\nThe reason why this all worked was because of CRAN. CRAN has (maybe forcibly) created a strong consensus on how to package and distribute R code, which means that nine times out of ten an R package will install and run with no user configuration. Today I am fairly comfortable at the command line and futzing around getting computer programs to work on my machine, but I’m still completely unwilling to use an R package that requires me to do much more than install.packages(\"package_name\").\nThis feature is important to me because I want my code to be useable and installable by people who, like my law school self, do not think of themselves as programmers and do not have any tolerance for command line bullshittery. My goal for all the products I develop is that they are installable and runnable with a single user action, and that action cannot take place in the terminal. This means that when I’m importing dependencies, I need to be confident that all of those dependencies themselves can be installed and set up without user intervention. CRAN does this by moving a lot of the setup and configuration pain from the end user to the package maintainer, and while this probably slows down development and release of packages, it vastly improves the user experience of the average user.\nPython is, well, not like that. I’m not a great Python developer, but I am a professional computer programmer and I still feel like it’s even odds that installing some Python library is going to cost me an afternoon of torture and four broken keyboards. If there are any Python evangelists still reading this, they might have a response that begins with, “Well you just,” but remember the user that I care the most about only has 20 minutes of attention and no real programming skill, so the only thing they can “just” do is copy and paste one line of code into a console. If that doesn’t work, I’ve lost them, and they’ll spend another lonely year renewing their SPSS licenses.\n\n\n4. Functional programming\nR is a functional programming language, which means that the natural way to accomplish something in the language is to use functions. I really like this pattern of programming because breaking complicated jobs down into small functional bricks gives me confidence that the overall solution is correct. I can work on the small functions, verify that they’re correct through tests, and then know that combining those building blocks together won’t change their behaviour.\nFunctional programming is not the best paradigm for all problems. For example, React is a functional paradigm for building user interfaces where you stack together functional components to build complicated web apps. The initial issue with React was that since the components were designed to be independent from one another, it made it very difficult to track application state. If a user logged in in some part of the application, you would have to pass that information back up to the top level of the app, and then back down to all the components that needed to know about it. It was easy to miss a connection, which would result in the user being logged into one part of the app, but logged out of another.\nThis creates a lot of difficult bugs where the application state is inconsistent across the product. React solved this problem by adding in global state stores like Rudux, which let people still use pure functional components for most things, but break that pattern when you need to set or access application state.\nFunctional programming is a great tool for data science problems because they are mostly stateless. When I’m building a statistical model, what I’m really doing is creating a mapping between some set of inputs and an output; in other words, a function. It’s usually more important that the mapping is clearly defined than that it takes into account user state. It is possible to do functional programming in Python, but it’s a bit like ordering soup at a pizza parlour.\nWhile a lot of the FP tools are there, the majority of the community doesn’t use functional patterns as their main development paradigm, and you’d probably get a few “That’s not Pythonic” comments on your pull request.\n\n\nConclusion\nNone of this is to suggest that anyone else should use R. R, like all other languages, is a bundle of trade-offs, and those are bad trade-offs in many contexts. In my context, however, the flexibility of R is extremely useful, and I can’t give up those language features without my work suffering."
  },
  {
    "objectID": "posts/2021-09-07-why-the-economist-s-excess-death-model-is-misleading/index.html",
    "href": "posts/2021-09-07-why-the-economist-s-excess-death-model-is-misleading/index.html",
    "title": "Why the Economist’s excess death model is misleading",
    "section": "",
    "text": "What got me started thinking about this subject was this tweet by one of the writers at The Economist suggesting that Kenya was radically undercounting deaths which have resulted from the Covid-19 pandemic.\n\n\nGlobal statistical modelling done by The Economist estimates that the true number of those who died in Kenya as a result of the covid-19 pandemic is between 19,000 and 110,000, versus an official death toll of 4,746. https://t.co/kLlhEmBtUa\n\n— Adrian Blomfield (@adrianblomfield) September 4, 2021\n\n\nBefore we begin it’s worth reflecting on the boldness of this statement. There are lots of smart people in Kenya whose whole job it is to count deaths, and the number they came up with is 4,746. Now of course this number could be wrong, but if you are going to suggest that it’s off by a factor of 20 you should have a lot of confidence in the model which generated those results and the data that that model is based upon. Sometimes models do produce surprising insights, but 99% of the time when your model produces an estimate that is intensely surprising to subject matter experts it’s because the model is wrong. Happily the Economist published their code and so we can look at it to see if these results make sense.\n\nFirst, articulate the mechanism.\nThe first thing I do when evaluating a project like this is to step back and ask whether the overall goal of the model makes sense, and in this case it doesn’t. The idea behind this model is that some countries are publishing excess deaths numbers and others are not, so we can use various features of the countries that are publishing the numbers to estimate excess deaths in the countries that are not.\nThis doesn’t make sense to me because the mechanisms of excess death are not at all similar between countries. I don’t see what the EU’s excess death really teaches us about Kenya’s. For example one of the effects of Covid was a shortage of hydroxychloroquine, which is used to treat malaria. This probably caused zero deaths in places where malaria isn’t a major cause of death, but probably did cause some deaths in other parts of the world. Similarly, one of the causes of excess death in rich countries was disrupted ambulance service, which wouldn’t apply to parts of the world that don’t have these services. If the causes of death differ dramatically across countries, why would we think that a single model is going to do a good job at estimating excess death across the world?\nWe can also use our understanding of how Covid works to inform excess death priors. For example, what happens if a country like Kenya were really only able to identify one in every twenty Covid deaths? When a country can’t identify Covid deaths it’s usually because it is unwilling or unable to to test for Covid, which results in people dying without knowing their Covid status. What we’ve learned is that identifying Covid cases is the basis for all Covid policy and if you can’t identify how many people have died from the disease you’re also not going to be able to do anything else to control it. If left uncontrolled, the disease will infect everyone in the country, and kill some proportion of those people. All this should lead you to think that there’s a kind of bimodal expectation for Covid outcomes: if the government is competent then the excess deaths should be close to reported deaths, and if they’re not then you should expect them to be n% of the total population where n is the age-adjusted infection fatality rate.\nThis expectation should make you suspicious of The Economist’s excess death model because it produces estimates that are both too high and too low. The estimate for Kenya is way too high if you trust the state capacity at all, and way too low if you think that all 50 million people in Kenya were infected with Covid and never ended up accessing medical care.\n\n\nUse interpretable models.\nThe Economist’s model is a gradient boosting machine (GBM) model with 160 (!!) features fit on data from about 80 countries (!!!). GBMs are powerful and accurate techniques for some problems, but I think it’s inappropriate for this one. This is a complex, non-linear model, which means that it’s difficult or impossible to understand exactly why it’s giving a particular answer to a particular case, and how its predictions would change if the parameters change. Gradient boosting machines are similar to neural networks, in that they can produce incredibly accurate models when provided the right problem and the right dataset, but can fall down embarrassingly when your data or problem definition is biased in some way. When you’re developing these models, you know how accurate the model is by some definition of “accurate,” but you never really know how the model works mechanically, and so it’s very hard to predict how it will behave on out-of-sample data. Additionally, this the model has a large number of features, and relatively few real samples which creates a big risk of over-fitting.\nTraditional statistical models like OLS or multilevel models are interpretable in the sense that by looking at the model definition you can understand exactly how it would behave in new situations and can judge whether that behavior is sensible. For example, if you were to fit an OLS model to The Economist’s data, you might discover that it thought that the more doctors per capita a country had the more excess deaths they should have. Looking at the coefficients of the model would alert you to this nonsensical conclusion and cause you to reevaluate the problem. When you use black-box models like GBMs you never get that feedback. Since most journalists use models to write stories about the world, they should almost always use interpretable models because they provide explanations in addition to estimates.\n\n\nInterrogate your training data.\nWhen you build these types of models in industry you spend a huge amount of time worrying about whether your dataset and problem definition accurately reflects the world. The nature of complex non-linear models means that they can produce weird results on cases that weren’t represented in the training data. This is what causes neural networks to do things like misclassify images when single pixels get flipped, or think that everyone’s a white person. Since the model isn’t able to tell you why it makes particular decisions, the only way to ensure that it’s going to perform well out of sample is to be sure that the dataset truly reflects the world.\nThe Economist’s dataset does not truly reflect the world. In particular it contains data from only four African countries: South Africa, Egypt, Tunisia, and Mauritius – three relatively well-off coastal countries and one tiny island. All of the one billion people who live in the 53 countries between Egypt and South Africa are excluded from the data, and importantly are not that similar to those countries that were included in the training data. You can look at GDP per capita and life expectancy to see how biased the data is:\n\nThe model only saw one lowish life expectancy country (South Africa) and one country with a per-capita GDP under $1,000 (Tajikstan). The only way you can think that this data says anything about countries in Central and Eastern Africa is if you think that things like life expectancy and income are totally unrelated to how people die from Covid, which is a stupid thing to think. The ways that people die in poor countries are very different from the ways that they die in rich countries, and so you really need poor countries in your sample if you want to estimate excess deaths in those countries.\nDespite this, The Economist went out and published a bunch of slick looking graphics about the “true death toll” in countries that are not remotely represented in the training data. These graphics provide a time series with some official-looking confidence intervals, and as far as I can tell the numbers are effectively manufactured by the modeling process. A complex non-linear model cannot produce reliable estimates for out-of-sample data that does not resemble the training data. Numerical estimates for these places are figments of the models imagination and have no tether to reality.\n\n\nDon’t assume the argument.\nOne of the most interesting questions of the pandemic is why Africa has been spared. Virtually every analyst thought that Africans would suffer horribly from Covid in the same way that they have suffered from virtually every infectious disease. Low state capacity, poverty, and poor medical care is a recipe for a high mortality rate. Reported deaths in Africa have, however, stayed extremely low.\n\n\nThis has only grown more surprising over time. Africa has extremely low vaccination rates relative to North America and Europe, less natural immunity, and the circulating variants are infectious enough that even advanced economies like Australia are having trouble containing them.\nIt’s possible that this is just a feature of reporting and that the true death rate in Africa is higher than that of North America or Europe, but it’s also possible that there’s an unknown factor that protects Africans from Covid. For example, it could be explained by vitamin D, cross-immunity from some prior infection, or some selection effect related to high infant mortality. The interesting question here is whether it’s a lack of reporting or something else, because that something else could hold the key to treating Covid.\nThe Economist’s excess death model kind of assumes away this question. The assumption behind this model is that there’s a consistent, known relationship between various indicators and excess death and so there are no significant heterogeneities between countries. This is an assertion and since they have no data from any of the countries that exhibit these oddly low death rates, it’s an assertion that can’t be tested.\n\n\nDon’t publish crappy estimates.\nThe response to these criticisms is probably something like “Well, this is the best we could do,” and I think that’s wrong. Models like this have the effect of putting a thin veneer of objectivity and science-y thinking over what’s basically an op-ed and most people do not have the skill to parse a modeling project and identify these problems. This is doubly true when you use a black-box model and don’t show your intermediate analysis. It’s irresponsible to publish these models without clearly communicating their limitations and suppressing predictions which you ought to know are pretty fishy. If you are not able to do that then the “best you can do” is not publish."
  },
  {
    "objectID": "posts/2022-08-04-dbplyr-testing/index.en-us.html",
    "href": "posts/2022-08-04-dbplyr-testing/index.en-us.html",
    "title": "Testing dbplyr packages",
    "section": "",
    "text": "“Good SQL” is contextual\nThe motivation for leaning SQL is that it will allow you to write effective queries. The idea is that if you know SQL very well you’ll be able to express your ideas more quickly, and your queries will execute faster. The problem with this is that because different databases differ in how they store data, and in which dialect of SQL they use, there’s not really a general way to write effective SQL queries. A query which is optimal for a row-oriented database like Redshift may do poorly on a partitioned columnar database like Snowflake. Moreover how a specific database is set up will dramatically change query execution speed.\nThis was driven home for me recently when I was writing wrapper functions for Socure’s new data platform. I asked the engineers on the project for advice about how to query the database and they explained that it’s not really possible to predict which query will run faster without testing it. The reason for this is that modern databases do a lot of query parsing to optimize the query which is sent by the user into something that can run efficiently on the database. These parsers are quite complex which makes it hard to predict which types of queries will run most efficiently.\nConsider the advice to use common table expressions (CTEs) instead of subqueries in a SQL statement. This is usually good advice because it leads to more readable SQL code, but it turns out that it can lead to 600% higher costs when run against some databases. There’s no single set of best practices for writing SQL because what counts as good and bad SQL depends on which specific database you’re querying.\nAs a result of all of this most data engineers try have more or less abandoned modifying user behaviour. Instead of asking the user to send just the right SQL, they modify the database to respond to the queries the user is actually sending. For example if a query is running too slowly, they may set up some kind of view, or change how the data is partitioned to make the query fast. The idea is that the user should write a query which is understandable to them, and the database should take care of\n\n\nWriting dbplyr wrappers\nA big chunk of my job over the last few years has been writing database wrappers to perform common queries, and whenever possible I try to use dbplyr for these functions. In general I have found that this approach has made it easier for me to write and maintain functions, and the queries that these functions generate are typically as or more performant than writing SQL directly. There are a few main reasons why I prefer this pattern.\n\nI have to write less code\nThe main reason I like using dbplyr functions is that I can leverage the rest of the dbplyr ecosystem to write less code. For example let’s take a look at the nycflights13 data and imagine that we wanted to write a function to get the mean arrival time per airport. Here are the dplyr and SQL functions I would write:\n\nlibrary(\"DBI\")\nlibrary(\"dplyr\")\nlibrary(\"duckdb\")\ncon <- dbConnect(duckdb::duckdb(dbdir = \"flights.duckdb\", read_only = TRUE))\n\nmean_dest_time_dbplyr <- function(con = con) {\n  tbl(con, \"flights\") |>\n    group_by(dest) |> \n    summarize(mean_time = mean(arr_time))\n}\n\nmean_dest_time_sql <- function(con = con) {\n  qry <- 'SELECT \"dest\", AVG(\"arr_time\") AS \"mean_time\"\nFROM \"flights\"\nGROUP BY \"dest\"'\n  DBI::dbGetQuery(con, qry)\n}\n\nWhen you start out these functions are about the same, but what happens when you start getting requests from users to add arguments to the function? For example maybe someone wants the function to allow you to filter by air time, if you’re patching SQL together you have to do something like this:\n\nmean_dest_time_sql <- function(min_time, max_time, con = con) {\n  where_clause <- paste0('WHERE (\"air_time\" >= ', \n                         min_time, \n                         ') AND (\"air_time\" <= ', \n                         max_time, \n                         ')')\n  \n  qry <- paste0(c('SELECT \"dest\", AVG(\"arr_time\") AS \"mean_time\"',\n                  'FROM \"flights\"',\n                  where_clause,\n                  'GROUP BY \"dest\"'\n                  ),\n                collapse = \"\\n\"\n  )\n  DBI::dbGetQuery(con, qry)\n}\n\nWriting functions like this is annoying because you spend a lot of time pasting queries together, but the bigger issue is that users will never stop asking for functionality. Accommodating these requests will lead to a bloated, complex function with a lot of arguments. Using dbplyr relives us of this complication by letting the user to pass any filter they want to our wrapper function.\n\nmy_tbl <- tbl(con, \"flights\") \nmean_dest_time_dbplyr <- function(tbl = my_tbl) {\n  tbl |>\n    group_by(dest) |> \n    summarize(mean_time = mean(arr_time)) \n}\n\n# They can filter by arr_time\nmy_tbl |> \n  filter(arr_time >= 100,\n         arr_time >= 200) |> \n  mean_dest_time_dbplyr()\n\n# But also by other stuff!\n\nmy_tbl |> \n  filter(month == 2) |> \n  mean_dest_time_dbplyr() \n\nThis pattern lets you leverage all of the dbplyr infrastructure which means that you have less code to maintain and less education to do.\n\n\nComposable database wrappers\nThe second main reason to use dbplyr is that it lets you write composable SQL functions. One of the great things about dbplyr is that it is smart enough to generate adequate SQL regardless of the order in which you call the function. For example putting the filter and mutate in different places will generate different SQL, but both queries will work.\n\n tbl(con, \"flights\")  |> \n  filter(month == 1) |> \n  mutate(long_flight = ifelse(air_time > 100, \"long\", \"short\")) |> \n  show_query()\n\n<SQL>\nSELECT\n  *,\n  CASE WHEN (\"air_time\" > 100.0) THEN 'long' WHEN NOT (\"air_time\" > 100.0) THEN 'short' END AS \"long_flight\"\nFROM \"flights\"\nWHERE (\"month\" = 1.0)\n\n tbl(con, \"flights\")  |> \n  mutate(long_flight = ifelse(air_time > 100, \"long\", \"short\")) |> \n  filter(month == 1) |> \n  show_query()\n\n<SQL>\nSELECT *\nFROM (\n  SELECT\n    *,\n    CASE WHEN (\"air_time\" > 100.0) THEN 'long' WHEN NOT (\"air_time\" > 100.0) THEN 'short' END AS \"long_flight\"\n  FROM \"flights\"\n) \"q01\"\nWHERE (\"month\" = 1.0)\n\n\nComposable functions are amazing because they let the user build complex expressions out of simple to understand components. For example let’s say that we wrote a function by_day that grouped the flights data by day.\n\nby_day <- function(tbl) {\n  tbl |> \n    mutate(date = paste0(year, \"-\", month, \"-\", day)) |> \n    group_by(date)\n}\n\nSince this function can be stacked along with other dbplyr functions it gives the user a lot of flexibility. They can stack it with other dplyr verbs in an arbitrary order, or even use it on an entirely different table, and everything will still work.\nSQL is generally speaking not composable. You can’t write small fragments of queries and easily insert them into other queries and so your wrapper tends to need to do more work. When you write wrappers with SQL query construction you end up trying to build a comprehensive function that limits the user to the things that you had in mind when you wrote it. Additionally you can’t easily share fragments across functions which means that you end up with repetitive code.\n\n\nBackend-agnostic functions\nFinally, one of the benefits of building dbplyr database wrappers is that your functions will run on a variety of data sources. For example a common pattern at my job is pulling a large set of data into an Apache Arrow Dataset for further analysis. Functions built around dbplyr will tend to work on these datasets without modification which reduces the number of things that the user has to learn or remember.\n\n\n\nTesting dbplyr functions\nI’ve been writing dbplyr wrappers for some time, but I’ve only recently come up with a testing pattern which I really. There are four main things that I want when testing database functions:\n\nTests should run without access to the actual database\nThey should allow me to test the output R object\nThey should include SQL assertions that I can use to communicate with the database owner\nI don’t want to regenerate mocks every time I change the function\n\nPreviously I would test database functions with dittodb which allows you to record mocks for particular SQL queries and cache the result of those queries. This accomplished goals 1-3, but over time I found the upkeep difficult. Because dittodb mocks the particular query you end up with a lot of mocks, and you need to regenerate them whenever the function changes.\nMy new approach is to record a mock of a few records from the whole database and store that as an on-disk duckdb database. In the test files I point my functions to the new database and run two types of tests:\n\nTest that the function produces the right output\nTest that the function generates the expected SQL\n\nFor example I would test that the by_day function produced the right output with a test like this:\n\nlibrary(testthat)\ntest_that(\"by_day function genrates the right output\", {\n  day_counts <- tbl(mock_con, \"flights\") |> \n    by_day() |> \n    count() |> \n    collect(n = 5)\n  \n  expect_s3_class(day_counts, \"data.frame\")\n  expect_equal(dim(day_counts), c(5, 2))\n  expect_equal(day_counts$date,\n               c(\"2013-6-26\", \"2013-6-27\", \"2013-6-28\", \"2013-6-29\", \"2013-6-30\")\n               )\n})\n\nThis gives future developers a clear understanding of what this functions is supposed to do, which lets them make changes with the confidence that they won’t violate the user expectations. I also want to generate SQL so that I can use it to communicate with the people who maintain the database.\n\ntest_that(\"by_day function genrates the right SQL\", {\n  expect_snapshot({\n    tbl(mock_con, \"flights\") |> \n      by_day() |> \n      count() |> \n      show_query()\n  })\n})\n\nWhen you run the test suite for the first time this will generate the expected SQL query which is used to test the function in the future:\nCode\n  show_query(count(by_day(tbl(mock_con, \"flights\"))))\nOutput\n  <SQL>\n  SELECT \"date\", COUNT(*) AS \"n\"\n  FROM (\n    SELECT *, CONCAT_WS('', \"year\", '-', \"month\", '-', \"day\") AS \"date\"\n    FROM \"flights\"\n  ) \"q01\"\n  GROUP BY \"date\"\nThis is an extremely useful test fixture for two reasons. First it gives you a something which can be easily shared with the database team. For example if you noticed odd results when running the query against the actual database you can send them the specific query which used to work, but now fails. You can even share this fixture with the database team to use in their tests. Secondly, it lets you lock down the expected query. This is useful if you do find out that some types of queries run better on your particular database and want to ensure that future developers don’t introduce bad query patterns.\n\n\nConclusion\nNo programming framework is comprehensive and there are plenty of times where it’s important to move past dbplyr and optimize the actual SQL that your functions generate. In general though I’ve found that starting with dbplyr saves me time and energy, and produces a better experience for the people who use my functions. Nine out of ten times dbplyr writes better SQL than I do."
  },
  {
    "objectID": "posts/2020-05-03-vitamin-d-and-covid-19/index.html",
    "href": "posts/2020-05-03-vitamin-d-and-covid-19/index.html",
    "title": "Vitamin D and Covid-19",
    "section": "",
    "text": "In a recent piece about the puzzling ways that Covid-19 has spread across the world the New York Times explores a number of possible theories about why Covid-19 has affected some countries more grievously than others, including “demographics, culture, environment, and the speed of government responses.” I think Vitamin D status should probably be included in this conversation."
  },
  {
    "objectID": "posts/2020-05-03-vitamin-d-and-covid-19/index.html#a-tale-of-two-countries",
    "href": "posts/2020-05-03-vitamin-d-and-covid-19/index.html#a-tale-of-two-countries",
    "title": "Vitamin D and Covid-19",
    "section": "A tale of two countries",
    "text": "A tale of two countries\nCanada and Australia have had pretty similar Covid-19 timelines. They both had their first case at the end of January, and hit a hundred cases around mid- March. They are similarly sized countries with similar demographics, and both have robust testing infrastructure. Despite this, Australia has 3.6 deaths per million people (to date), while Canada’s rate is twenty times as high.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCountry\nCases\nDeaths\nPopulation\nDeaths per million\nDeaths / Resolved cases\n\n\n\n\nAustralia\n6,744\n89\n25,000,000\n3.6\n2%\n\n\nCanada\n51,150\n2,983\n37,000,000\n80.6\n13%\n\n\n\n\n\n The story that you’re likely to hear from the Australian government is that their response was timely and effective, and because of this they were able to prevent widespread infection. However, when you look at the timelines of policy changes between the two countries they’re quite similar. They were both adjacent to large outbreaks. Canada shares a border with the United States, while Australia had direct, un-screened flights from Wuhan until late January. They both enacted screening and quarantine policies which were unable to wholly prevent disease importation or community spread, and they both enacted similar social distancing regulations around the same time. For example Australia cancelled university classes in the third week of March, while most Canadian provinces closed schools in mid-March.\nDespite all this, the countries ended up with extreme differences in death rates. I live in Nova Scotia, a small province of 900,000 people, which has had almost as many deaths (31) as New South Wales (39). NSW has eight times as many people as Nova Scotia, relatively higher density, and an outbreak that started two months earlier.\nEven if we accept that the Australian response was much better than the Canadian one, that would likely account for differences in infection rate. But there’s still a large difference in the death rates between the two countries. In Canada, 13% of resolved cases have resulted in deaths, while in Australia just 1.3% of resolved cases are deaths. Both countries have high quality medical systems that weren’t overwhelmed by the disease. There’s no real treatment for Covid-19, so public action shouldn’t be able to influence the course of the disease.\nIt doesn’t make sense that people in Canada are dying at a rate 10 times higher than Australia. So what the hell is going on?\nThere are a few good theories for the difference including:\n\nAustralia’s Aged Care system preventing infection in the very old\nHeat preventing viral transmission\nLuck"
  },
  {
    "objectID": "posts/2020-05-03-vitamin-d-and-covid-19/index.html#the-case-for-vitamin-d",
    "href": "posts/2020-05-03-vitamin-d-and-covid-19/index.html#the-case-for-vitamin-d",
    "title": "Vitamin D and Covid-19",
    "section": "The case for vitamin D",
    "text": "The case for vitamin D\nOne theory that hasn’t gotten enough attention, though, is that vitamin D plays an important role the immune response to Covid-19. Most people get their vitamin D through their skin, and if you are far from the equator, your skin can only manufacture vitamin D during the summer. One study of 3.4 million blood samples collected in the U.S. over the course of five years showed that vitamin D levels peaked in August and bottomed out in February. Since the pandemic started at the end of Canada’s winter and the end of Australia’s summer, average vitamin D levels would have been much lower in Canada and higher in Australia.\nThere’s some preliminary (preprint) research that supports this theory:\n\nA restrospective study in the Philippines found a significant association between vitamin D status and severe Covid-19 infections.\nAn Indonesian study of 712 people found that after controlling for age, gender, and co-morbidity, vitamin D deficiency was associated with a twelve times higher risk of death.\nUVB radiation, which the skin uses to manufacture vitamin D is associated with lower death rates and case fatality rates.\nCovid-19 appears to vary by latitude.\nHospitalized male Covid-19 patients were found to have lower vitamin D levels than controls\n\nThere are some additional factors that make vitamin D a plausible candidate:\n\nVitamin D helps regulate the renin angiotensin system which includes the ACE2 receptor that Covid-19 binds to.\nVitamin D may bind to the non-structural protein nsp7 on the SARS-CoV-2 virus.\nVitamin D deficiency is common among populations at risk for Covid-19 mortality, including people of colour, the elderly, people with a high BMI, health care workers, and those in institutional settings.\nVitamin D reduces the risk of other acute respiratory infections.\nVitamin D deficiency is very common in places that have been hit hard by the disease.\n\nLastly, vitamin D status would help explain some other puzzling phenomena:\n\nWhy hasn’t the disease been worse in equatorial countries with relatively constrained public health resources?\nWhy are people of colour at such a high risk for severe Covid-19 in northern latitudes?\nWhy is Louisiana doing better than New York?\nConsidering the age demographics in Florida, why isn’t Florida doing worse?\n\nNow, I’m not saying that vitamin D is definitely the cause of differential Covid-19 mortality; it’s quite likely that there’s some bundle of factors that explain all of the above. But given that it’s an abundant, safe, and well understood therapy, I think we should be paying attention to it. I’m looking forward to the results of some of the vitamin D clinical trials and am concerned that countries in the southern hemisphere are going to see worsening outbreaks as the seasons change."
  },
  {
    "objectID": "posts/2017-08-29-learning-data-science/index.html",
    "href": "posts/2017-08-29-learning-data-science/index.html",
    "title": "“Advice for non-traditional data scientists”",
    "section": "",
    "text": "When I started, I honestly didn’t have any particular skills or capacity which would have made data science a good career choice. I studied philosophy in undergrad, and while I had done a bit of statistics, it wasn’t something I would have said I was comfortable with. All I really had was an interest and the capacity to learn new things. If you’re in a similar boat, here is some advice about the process:\n\nEmulate one or two people who know what they’re doing.\nThere is a huge diversity of tools and techniques for approaching data work, and if you half-learn a lot of different techniques you won’t be able to fully understand or accomplish any one technique. My recommendation is to pick one or two people who work in the field and who speak in a language you understand and try to emulate them. In my case I really focused on learning the R programming language, and picked a few R programmers to follow. I listened to all of their online talks, read their blogs and follows their activity on Github. This meant that I ended up with a deep understanding of a few small areas of the language and missed out on a lot of other areas. For instance, I learned dplyr really well, but didn’t learn much about object oriented programming. It’s good to try to develop depth of knowledge, rather than breadth, because when you know one thing very well you can usually apply that knowledge to other areas. A shallow understanding of many areas won’t help you tackle advanced problems in a specialized area.\n\n\n“Learning to code” and “learning statistics” are terrible goals because they have no end point.\nWhen you are learning a new skill, it’s important to have specific criteria for success. This helps keep you on track and also helps mitigate imposter syndrome. You don’t want to move the goalposts as you develop your understanding. From this perspective, “learning to code” and “learning statistics” are terrible goals, because there’s always more to learn about these fields. It’s better to have smaller goals, like, “Learn to write a function in R,” or, “Be able to fit a linear model,” because those things can be accomplished. Goals that can be accomplished are good things because you can accomplish them, rather than being constantly reminded how far you have to go.\n\n\nFocus on trajectory\nWe naturally compare ourselves to others and tend to judge our own skills in terms of other people’s skills. The problem with this is that as our understanding improves, we tend to change our measures of comparison to more and more accomplished people. This is especially a problem when we compare our own general understanding of an area to that of specialists. For instance, you might have a good general understanding of neural networks, but if you compare yourself to someone who studies them full time, your understanding will obviously be pretty paltry. This kind of comparative thinking leads to always feeling insufficient, because no matter who you are or how much you know, there is always somebody who will know more.\nA better approach is to focus on trajectory. Ask whether you are making progress rather than whether you are relatively successful. Think about what you knew yesterday and feel good if you learned a bit more today. Over time that approach will lead to much better understanding with much less suffering.\n\n\nFollow Kind Experts\nEvery field has experts, and many of those experts are assholes. Indeed in our society we frequently use lack of empathy or kindness as a sign of intelligence or accomplishment. We call these people “geniuses,” or “rockstars,” and try to forgive their personal faults by pointing to their intellectual accomplishments. I think you should ignore these people and instead try to find experts who have a genuine concern for other people. There are two reasons for this.\n\nKind experts will help you learn. This is common sense. If somebody has a genuine concern with helping other people understand an area, the resources they produce will be better at teaching other people how to do that thing. Moreover the community of learners that surrounds these people is going to be supportive, rather than combative, and so engaging with that community will be a good experience\nKind people know more than unkind people. This is less intuitive, but just as important. Experts who are genuinely concerned with other people tend to create environments where other people help them. A good example of this is Hadley Wickham’s Twitter feed. What you notice following him is that he is for the most part very kind in how he communicates with other people. The result of this is that he has developed a powerful network of people who answer his questions, test and provide feedback on his software projects, and grow into kind experts themselves. Most of the really productive developers I have interacted with have these same kinds of networks, and the reason they have them is that they spend a lot of time and energy supporting people, rather than belittling them. No matter how brilliant a brilliant jerk is, they will always always lose out to a group of people working together on a problem.\n\n\n\nTry to Ignore Boundary Setting Behaviour\nBoundary setting behaviour is when people who are part of a group attempt to draw the lines around that group to include themselves and exclude you. For instance, programmers sometimes say things like, “Real programmers use the command line,” or, “You really need to learn Scala if you want to be a programmer.” The motivation for this is not to accurate express the boundaries of the discipline, but instead to make themselves feel better about their own skills. Often, out of insecurity, people will express the importance of their own skills and try to minimize the importance of skills they lack. About half of the stuff you read is written to address that insecurity rather than to help you learn. If possible, you should try to avoid this kind of advice.\nBut you will definitely encounter it. You will get rejected from jobs, or made to feel like an idiot, because of this kind of behaviour, and there’s nothing you can really do about that. Applying for data science jobs from a non-traditional background, I frequently encountered people who believe all of the following:\n\nData manipulation, visualization, and communication are the most common data science tasks;\nWe should focus on building teams with distinct capacities rather than trying to find one person who knows how to do everything;\nMost of the time you want a simple linear model, not a complex machine learning algorithm;\nThere is a labour shortage for data science roles and we should expand the applicant pool; and\nIt is absolutely essential that you have an advanced degree in statistics to apply for a job at my company\n\nNow, there’s a way that all of these things can be true at the same time, but more likely than not #1-4 describe a lot of the actual job requirements, and #5 is boundary setting. But when you apply for and get rejected from the job you can’t really ignore that behaviour; you have to deal with it emotionally.\nOne of the key ways you can recognize boundary setting behaviour is when people start to equate being a member of a profession with being a particularly good member of that profession. For instance someone might tell you that you be a real data scientist you need to have a PHD in statistics, and have mastered R, Python, and big-data query languages, and be an exceptional written and verbal communicator. Having these skills probably makes you an extremely skilled data scientist, but are they really hard boundaries around the profession? I’m not so sure. In most cases we talk about jobs based on the job title, rather than the job requirements. If you’re a baseball player who stands near first base we call you a first baseman, if you write for a living we call you a writer. These things are true even if you write trashy science fiction novels or are a terrible fielder. The boundaries of the profession are set by the market, not by your skills, and so you can be a good or bad example of the profession without having that change your membership in that profession. I think the same thing should be true of programming. Can you get a job writing computer programs? Then you’re a programmer. Do you work with data for a living? You can probably call yourself a data scientist.\n\n\nLearn to Bounce Back\nThe main currency in learning a new skill is enthusiasm. Each time you have some success, you bank a bit of enthusiasm, and each time you experience challenges you lose a little bit of it. It is important to try to understand what helps you gain energy and enthusiasm, and what helps you lose it. Whenever you notice that you’re going into the red in the enthusiasm department, either take a break, or do something that helps you feel a bit better. Here are some of the things I do when I feel bleak about my abilities:\n\nTake a walk\nTake a nap\nExercise for five minutes\nAnswer easy stack overflow questions\nIf I’m frustrated by a computer problem I copy and paste the examples and run them\nMeditate\n\nI think the key to this is not so much what particular things you do to bounce back, but that you recognize when you are losing energy for a task, and take a break from learning.. Stress basically obliterates our ability to learn new things and so simply stopping for a bit is a powerful way to maintain our capacity for new skills."
  },
  {
    "objectID": "posts/2020-05-23-race-and-covid-19/index.html",
    "href": "posts/2020-05-23-race-and-covid-19/index.html",
    "title": "Race and Covid-19",
    "section": "",
    "text": "In a recent interview, Linda Villarosa outlines the three major causes that she and other public health researchers have identified as causes for the huge racial gap in Covid mortality:"
  },
  {
    "objectID": "posts/2020-05-23-race-and-covid-19/index.html#controlling-for-social-determinants-of-health",
    "href": "posts/2020-05-23-race-and-covid-19/index.html#controlling-for-social-determinants-of-health",
    "title": "Race and Covid-19",
    "section": "Controlling for social determinants of health",
    "text": "Controlling for social determinants of health\nOne of the main strategy statisticians use to identify whether a factor is important is to control for confounding factors. For example imagine if you had a dataset that showed a significant relationship between maternal weight and a child’s birth weight. If you had a suspicion that gestational time had some effect on both birth weight and maternal weight, you could include the length of the pregnancy as a term in the model. If maternal weight is just a spurious correlation, it won’t be a significant contributor to the model. The basic idea is that if a parameter is an important feature of the model, then either there is a real correlation between the feature and the outcome variable, or we’ve missed some other important factor that would explain away the relationship.\nI think we should assume at the start that the virus itself is not making racial distinctions. Genetic factors, like sickle cells or blood type can change how a virus attacks a person, but there’s probably no common genetic pattern that unites all black and brown people in Northern countries. As a result, it’s hard to imagine a mechanism whereby the virus itself is picking out people of colour. Whatever effect race has on mortality is probably mediated by something like medical bias or proximity to the virus, and so when a study says that race is a significant predictor after controlling for x, y, and z, that means that we’re probably missing an explanatory feature.\nThere are two main studies that have attempted to control for social determinants of health when explaining Covid-19 outcomes:\nFirst, a study out of the UK looked at NHS records and Covid-19 mortality. They controlled for preexisting conditions as well as a deprivation index that combines income, employment, premature death, education, crime, barriers to housing, and living environment. What they found is that after controlling for all of these factors, Black, Asian, and mixed race people had about twice the Covid mortality risk as white people.\nThis study confirms two out of the three causal pieces of Villarosa’s model of Covid mortality: proximity to the virus and pre-existing conditions. The deprivation index, which captures at least some of the socioeconomic factors that lead people to be more proximate to the virus, was associated with increased mortality, and pre-existing conditions were also found to lead to increased mortality. But since race still provides a lot of information, we know that there’s a missing factor.\nThe second study is a study of US veterans, which found that “Black and Hispanic individuals are experiencing an excess burden of Covid-19 not entirely explained by underlying medical conditions or where they live or receive care.” The main outcome of this study was positive Covid diagnosis, and it controlled for age, pre-existing conditions, and whether someone lived in an urban or rural setting. They found that Black veterans were more likely to be tested than white veterans, but were about twice as likely to test positive.\nThis study also controls for two pieces of Villarosa’s model: pre-existing conditions and healthcare bias. It directly controls for pre-existing conditions, but there also is a kind of implicit control on medical bias because the main outcome was a positive test. Logically, healthcare bias is more important the more treatment decisions a caregiver has to make. Giving birth is a great example: a doctor needs to decide whether to admit someone who might have a dangerous pregnancy, whether to induce, how to monitor fetal heart rate, and at what point to intervene with an emergency C-section. Bias creeps in at all of these points (and more); doctors might send a Black woman home when they would admit a white woman, monitor fetal heart rate less carefully, or intervene in dangerous labour a few minutes later. Each of these biased decisions endangers mother and child, and it’s a big part of why there’s a big racial gap in infant mortality.\nBy contrast, Covid diagnosis really only involves the one decision of whether to test or not, and we can assume that racial bias would tend to lead doctors to undertest Black people. A doctor might do a Covid-19 test on a white patient with marginal symptoms, while they may send a Black patient with the same symptoms home without a test. So bias in this case would reinforce, rather than undermine, the finding that Black and Hispanic people are at a higher risk of Covid-19 diagnosis, because the more bias there is, the fewer tests they would receive.\nNeither of the studies above perfectly controls for the features in Villarosa’s model. It’s possible that the missing piece in the NHS model was biased medical care and that the missing piece in the VA model was proximity to the virus, but it should at least make you suspicious that there could be an important factor that isn’t being accounted for in the standard way of thinking about racial gaps in health outcomes."
  },
  {
    "objectID": "posts/2020-05-23-race-and-covid-19/index.html#mendelian-randomization",
    "href": "posts/2020-05-23-race-and-covid-19/index.html#mendelian-randomization",
    "title": "Race and Covid-19",
    "section": "Mendelian Randomization",
    "text": "Mendelian Randomization\nA recent study provides a strong argument that the missing factor is vitamin D. The study uses a technique called mendelian randomisation, which requires a bit of explanation: In the ideal world, we would answer the question of whether vitamin D deficiency increases your risk of dying of Covid-19 by controlling for vitamin D in the model, but we don’t have information about everyone’s vitamin D levels. Mendelian randomization is the idea that if you have information about something that you know is related to the thing you wish you had data on, then you can include that as a proxy. In this case we know that people with darker skin tend to have lower vitamin D levels than people with lighter skin, and that that probably gets worse as you move north. The intuition here is that the skin’s ability to synthesize vitamin D is less important in Florida than Wisconsin because intense UVB radiation is abundant for most of the year. So this study uses race + latitude as a proxy for vitamin D levels.\nThe rules of mendelian randomization are that the randomizing factor can’t be directly associated with the outcome, and it can’t be associated with other explanatory factors.\nSo if you think the causal graph looks like this, mendelian randomization is appropriate:\n\n\n\n\n\n\nThe BMJ study found that excess mortality went up as you went north. Places like Alabama, which we typically think of as having bad racial health outcomes, had lower excess Covid mortality than places like Wisconsin, which we generally assume have at least somewhat better racial health outcomes. This is a pretty good argument for vitamin D’s involvement because it’s hard to find a story where latitude modifies other inputs into the Covid mortality. This isn’t to say that there’s no such story, but the most obvious one is that vitamin D deficiency increases risk of Covid-19 complication. This is the main graphic from the paper:\n\n\nDoes latitude modify other social determinants of health?\nMendelian randomization is not appropriate if the proxy also acts on your other predictor variables. For instance, you might think that because there are fewer Black people in Wisconsin than in Georgia, there are also fewer Black doctors, and so implicit racial bias might be worse. If that were the case, latitude wouldn’t give you much information about vitamin D because it could also be acting on the other social determinants of health, which we already know are important.\n\n\n\n\n\n\nOne quick check we can do to see if this graph makes sense is to look at infant mortality data by latitude. If excess infant mortality were higher in northern states than in southern ones, that would suggest that latitude acted on social determinants of health, because things like medical bias are likely affecting black mothers in the same way they are affecting black Covid patients. This data from the CDC shows that children of Black or African-American mothers die at a rate 2-3 times higher than the children of white mothers, and this rate doesn’t really change by latitude.\n\n\n\n\n\n\n\nDoes latitude act directly on mortality?\nThe other way that mendelian randomization can be invalid is if the proxy acts directly on the outcome variable. In this case, that would mean that latitude directly increased excess Covid mortality. At first glance it’s hard to think of how this would happen, but if you add testing to the model it becomes a bit clearer. If southern states have worse testing, and in particular are less likely to test Black people, Covid deaths would be misclassified as some other kind of mortality. The result would be that excess mortality would look lower, because Covid deaths among Black people were being under-counted.\n\n\n\n\n\n\nI don’t have access to any data that shows testing by race and state, but we can look at the overall testing rate to see if that varies by latitude. The intuition here is that in places with abundant testing, racial bias would be less of a factor than in the places with scarce testing. Again, there doesn’t look to be much of a relationship here:"
  },
  {
    "objectID": "posts/2020-11-21-how-to-make-good-decisions-from-bad-data/index.html",
    "href": "posts/2020-11-21-how-to-make-good-decisions-from-bad-data/index.html",
    "title": "How to make good decisions from bad data",
    "section": "",
    "text": "What do we get from a clinical trial?\nThe purpose of clinical trials is to estimate the distribution of an outcome. You design and conduct an experiment, and based on the results and some statistical assumptions, you can estimate the range of outcomes that would be seen in the world. All the things we use to talk about clinical results – like confidence intervals, p-values, or statistical significance – are just ways of reporting this distribution, which we can visualize with a plot like this one:\n\n\n\n\n\nThese distributions have a mean, which is usually the estimated treatment effect, and standard deviation which tells you something about how certain you should be in the results. The reported distribution is, however, only half the story because it only accounts for quantifiable error.\nAll experimental data has two types of error, quantifiable error and unquantifiable, or irreducible error. Quantifiable error is basically the known unknowns of experimental data. For any dataset you have some estimate of how likely that data was to arise due to chance based on the variance of that data, and assumptions about the true distribution in the world. This lets researchers report a range of values which represent what you would expect to see if the trial were replicated many times.\nHowever, quantifiable error is not the only, or most important, type of error.\nThere are often problems with trials which relate with the way the trial was designed, structured, or executed. For example there could be a systematic, unknown error with a particular test, or Excel could have turned a bunch of genetic codes into dates, or maybe the trial was just entirely fraudulent. All of these things are sources of uncertainty, and importantly, we cannot estimate them from a single dataset because those estimates will never include the risk that the data itself is flawed. The people who undertake the study will do their best to prevent these problems, but the risk of them is always there.\nThe result is that for any study we really have two distributions of effects, the reported distribution which includes just the quantifiable error, and the true distribution which includes the unquantifiable error as well.\n\n\n\n\n\nWhat makes clinical trials special is that they are able to isolate a single mechanism from a broad range of confounders, and so the reported distribution is usually pretty close to the true one. Observational or ecological data can’t do this effectively, so the true distribution is usually much wider than the reported one. An observational study might report a highly significant result, but because there are lots of potential causal mechanisms which are not controlled for, it’s a poor estimate of what’s actually going on in the world. The result of these confounders is that the numbers reported by the study will usually be overly precise. The true distribution of effects is much larger than that reported by the study.\n\n\n\n\n\nThe important thing is that all evidence – whether it be experimental, observational, or ecological – has this problem. What generally makes randomized trials “good” and ecological trials “bad” is that for clinical trials the real distribution will be close to the reported distribution, while for ecological study it will be much wider. This is not a categorical distinction. There are well designed observational studies that provide an accurate representation of the world, and poorly executed clinical trials that do not. All of these trials have flaws, so you shouldn’t just believe any of them, and they all provide information, so you shouldn’t ignore them either.\nGiven that every study is an imperfect picture of the world, how can we make decisions based on them?\n\n\nReplicate, triangulate\nOne of the most important things to look for when evaluating a trial is whether it’s been replicated. Replication makes it much more likely that a trial is a good picture of the world because most causes of irreducible error are eliminated if you have several different research groups produce the same result.\nFor example, it’s possible that one researcher fabricated data, but it’s vanishingly unlikely that several of them did so in the exact same way. As a result, if two, relatively independent, trials replicate the same results it is more likely that those results are real and not due to mistake or malfeasance.\nReplication cannot, however, reveal systematic issues with the trial design, because those issues will be propagated to all the places where it was replicated. I’m going to illustrate this with my favourite example of whether vitamin D deficiency causes severe Covid-19 complications.\nThe first papers to suggest that vitamin D caused severe Covid were observational papers. The study design was to look at hospitalized patients and ask whether vitamin D levels were associated with Covid 19 progression, and they found that they were. Since then this result has been replicated in 18 other papers. This is great, but it doesn’t help our interpretation that much because these papers all share the same problems. Since these papers measured vitamin D levels after infection, we don’t know what caused what. It’s possible that vitamin D deficiency causes Covid, it’s possible that Covid causes vitamin D deficiency, and it’s also possible that some unknown factor causes both of them.\nTo put it visually, the data is equally consistent with the following three causal diagrams, and so replicating it doesn’t give us much information on what we should do.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTriangulation is basically the process of looking at a diverse set of study designs and asking what causal theory is consistent with all of them. The statistical insight here is the same as that underlying ensemble models and polling averages: If you have a bunch of diverse sources of evidence, their signal will be correlated but the error will be uncorrelated so if you take a global perspective on the data, the error and bias of the individual trials will tend to cancel each other out.\nIn addition to papers mentioned about there are also four observational studies which found an association between pre-infection blood samples and Covid-19, which makes the reverse causal model less likely. Since vitamin D levels measured before a person became sick are also correlated with Covid-19, it’s less likely that Covid was causing vitamin D deficiency.\nWe also have six studies, including two randomized clinical trials, which found that vitamin D supplementation improves disease progression in Covid positive patients. The two trials found that supplementation reduced ICU visits, and increased the proportion of mildly symptomatic people who were able to clear the virus.\nNeither of these randomized trials are definitive, but they are enough to rule out the confounder model. The confounder model is the idea that the association between vitamin D deficiency in the observational studies was due to some unknown confoudner that was causing both vitamin D deficiency and Covid-19 severity. This may be something like spending less time outside, or having a condition which both reduces your vitamin D levels and also increases your Covid jeopardy. Randomization, even with small sample sizes, will control for these unknown confounding factors because they will be randomly distributed between the treatment and control arms. Additionally, the confounder model is inconsistent with the idea that Covid risk can be treated with vitamin D supplementation. If that model were true then these trials should either show no effect, or at least a fairly small effect, rather than the large ones that they did in fact report.\nThis approach also lets you incorporate evidence from contradictory trials without necessarily needing to discard the information provided by other studies. For example just last week there was a 240 person randomized controlled trial that found that a single high dose vitamin D supplement given 10 days after symptom onset did not improve the outcomes of severely ill Covid patients. If you were evaluating this using heuristics, you might just look at the sample size of this trial and declare it the winner because it’s larger than the other two trials. In other words, you would pick the strongest piece of evidence, believe it entirely, and discard any other pieces of evidence. This is a bad idea because the small trials, and the observational data all provide some information, and when making decisions we should try to avoid throwing information away. A better approach is to at least try to incorporate this evidence together and see if there’s a plausible causal story that fits all the data points. It’s unlikely that all but one of the studies are totally wrong, so if there’s a causal story which incorporates all of the evidence, that’s more appealing than a winner-take-all approach where the best study is believed completely while the others are discarded. In this case, one way of reconciling the large failed RCT with the small, successful ones is that vitamin D is not effective in severely ill patients during Covid’s inflammatory phase, but is effective during the pre-infection and viral replication phases.\n\n\nThink about expected value\nThe second way to make good decisions based on bad data is to consider the expected value of a decision. Remember, what makes bad studies bad is that they don’t do a good job of estimating the real world value of an intervention, and so they can’t provide a precise estimate of what’s going to happen in the world. However, for many interventions we often don’t need a precise estimate to make a good treatment decision.\nConsider the idea of wearing a cloth mask to prevent Covid-19 infection. The evidence in favour of this intervention is really bad. We have a few mechanistic studies which show that, in laboratory conditions, a cloth mask can reduce the transmission of viral particles, and we have severalf highly confounded ecological studies that show that places that have a mask mandate (as well as many other things) have a reduced Covid disease burden. There is also a recent randomized study of mask usage, which failed to show that giving people masks reduced their risk of contracting Covid-19. If we go back to our distribution plots, the mask evidence might like this; there’s some reported benefit but the available data really can’t provide that precise an estimate that masks work:\n\n\n\n\n\nWhen you incorporate other information about the costs and benefits of mask usage, however, it becomes clear that the only good decision is to recommend masks. In particular, we have very good evidence that wearing a mask isn’t really harmful. Doctors, nurses, and dentists all wear masks all day, and do not suffer ill effects. Mask wearing is common in many Asian countries and there aren’t any reports of ill effects. While we’re not really sure about the upside of mask wearing, we can be very sure that there is no downside. Incorporating this information leads to a graph like this:\n\n\n\n\n\nThe important thing about this graph is that it allows us to make a decision based on the evidence even if we think that the evidence is terrible. While we can’t estimate precisely how much masks help, we can be pretty sure that they aren’t going to hurt us. The upside is some positive number, the downside is pretty much zero, and so the expected value of mask wearing will be positive. From the data we can’t tell how positive, but we really don’t need to know that, since Covid is extremely serious we should be taking every action that has a positive expected value, even if that value is pretty small. The poor quality ecological data and mechanistic studies reveal that it’s plausible that masks reduce Covid transmission, and that plausibility is all we should need to adopt the intervention.\n\n\nConclusion\nWe have no choice but to make decisions based on the data that’s in front of us. We’re lucky if we’re able to make those decisions based on several strong clinical trials, but more often than not the only information we have is based on small, biased, confounded studies. However, if we think through the totality of the evidence, as well as the expected value of our actions, we can usually make high quality, calculated decisions instead of waiting for more evidence."
  },
  {
    "objectID": "posts/2018-05-01-model-testing/index.html",
    "href": "posts/2018-05-01-model-testing/index.html",
    "title": "“Testing machine learning models with testthat”",
    "section": "",
    "text": "Automated testing is a huge part of software development. Once a project reaches a certain level of complexity, the only way that it can be maintained is if it has a set of tests that identify the main functionality and allow you to verify that functionality is intact. Without tests, it’s difficult or impossible to identify where errors are occurring, and to fix those errors without causing further problems.\nData science projects tend to be pretty under-tested, which is unfortunate because they have all of the same complexity and maintainability issues as software projects. For instance, if the packages you use change, the data changes, or you just go back and try to make some changes yourself you run the risk of breaking your analysis in ways thatcan be very difficult to detect. There’s been a fair amount of excellent writing on testing functionality and testing data but one of the best types of data science testing is to test the statistical model itself.\nStatistical models are often developed under rigorous observation but then deployed into a production system where they might not be monitored as closely. During the development process a data scientist will hopefully spend a lot of time interrogating the model to make sure that it’s not over-fitting, making use of bad data, or misbehaving in some other manner - but once the model is deployed they probably move on to other things and might not worry too much about whether the model is continuing to function as they expected. This leaves the model open to two big classes of bugs:"
  },
  {
    "objectID": "posts/2018-05-01-model-testing/index.html#what-do-tests-offer",
    "href": "posts/2018-05-01-model-testing/index.html#what-do-tests-offer",
    "title": "“Testing machine learning models with testthat”",
    "section": "What do tests offer?",
    "text": "What do tests offer?\nThe main benefit of testing software is that it helps clarify what exactly you expect from your code. Whenever we write a line of code we have some fuzzy sense of what that code should do but that sense isn’t super precise. We might look at the result of a function and say “that looks a bit fishy” or “that looks realistic” without stopping to clarify why it looks fishy or why it’s realistic. When you go to write a test you have to be more specific, which helps to make your thought process more precise. For instance, we might know that a person’s height can’t be too small, but in order to write a test you need to specify what “too small” means.\nThe same is true for modelling. There are lots of ways to get a sense of whether your model is fishy or not during the model development process. You might check how the model performs on new data, or check colinearity of its features. You could check whether the model performs well on easy cases, or make sure that the predictions aren’t too good to be true. The key point about testing models is that you should clarify and write down your expectations of the model. How should it perform on these different metrics? When should you start worrying about it?"
  },
  {
    "objectID": "posts/2018-05-01-model-testing/index.html#questions-to-ask-about-your-model",
    "href": "posts/2018-05-01-model-testing/index.html#questions-to-ask-about-your-model",
    "title": "“Testing machine learning models with testthat”",
    "section": "Questions to ask about your model",
    "text": "Questions to ask about your model\nThe answers to these questions are domain- and technique-specific. An analyst will ask very different questions depending on what they are modelling, the particular techniques they are using, and their background. Here are a few examples of tests, to hopefully get you started testing statistical models.\nLet’s start with a simple model of house prices from Kaggle.\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(testthat)\nhouse <- read_csv(\"house_prices.csv\")\nmodel <- lm(SalePrice ~ LotArea + Neighborhood, house)\nhouse$pred <- predict(model)\nrmse = function(predicted, observed){\n  sqrt(mean((predicted - observed)^2))\n}\n\n\nIs it performing well enough?\nThe most basic question you can ask about a model is whether it’s continuing to perform well as new data comes in. Typically when you develop a model for production you have made some kind of promise to your company or customer about how that model performs. For instance, it might be worth predicting a customer’s next purchase if the model is 50% accurate, but not worth doing so if it is only 2% accurate. A model can perform well on the training data, but if customer behavior changes in some way its accuracy can degrade. Instead of being notified of these changes by angry product managers, you can write a test that tells you when the model accuracy falls below a certain threshold. In this case we have a root mean squared error of 5.1891341^{4} dollars so let’s say we want to worry about the model when that error gets bigger than $75,000. I’m going to generate new data by sampling my existing dataset, but in production you should use actual new data. The test would look something like this:\n\nlibrary(testthat)\nnew_data <- house[sample(1:nrow(house), replace = TRUE), ]\ntest_that(\"model rmseis above threshold\", {\n  new_data$pred <- predict(model, newdata = new_data)\n  expect_true(rmse( new_data$pred, new_data$SalePrice) < 75000)\n})\n\nIt’s often helpful to check the other side of model performance as well to ensure that the model isn’t performing unnaturally well. This can happen in real life systems as users learn to game machine learning models; for example, if you have a model of student performance that rewards teachers based on how their students are doing, over time student and teacher behavior might start to shift to accord with the model. A good place to start with these kinds of tests is the intuition that your model performance should probably be worse in the wild than it was when you first developed it. If it starts improving you probably want to do some investigation to understand why.\n\ntest_that(\"model rmse isn't too hight\", {\n  expect_true(rmse( new_data$pred, new_data$SalePrice) > 20000)\n})\n\nThis approach is flexible in that you can test all of the model performance metrics which you used to develop your model. In fact you can write these tests out before you develop the model and use them to test the model as you develop it. This can be a helpful way to prevent yourself from cherry picking measures of performance that look good for your particular data.\n\n\nAre the predictions within a sensible range?\nA common form of sanity check for a model is to generate some predictions and check that they are within a sensible range. For instance we might want to check that we’re not predicting negative house prices, or house prices which are extremely large.\n\ntest_that(\"model predictions are sensible\", {\n  expect_true(min(new_data$pred) > 0)\n  expect_true(max(new_data$pred) < 2 * max(house$SalePrice))\n})\n\nDepending on your domain you might also want to check if the model is predicting outcomes that weren’t really part of the training set. For instance our model didn’t see very small or very large house prices so we might have some issues with those predictions. These tests check that most of the model’s predictions are within the range of our training data.\n\ntest_that(\"model predictions are within training range\", {\n  small_predictions <- mean(new_data$pred < min(house$SalePrice)) \n  large_predictions <- mean(new_data$pred > min(house$SalePrice))\n  expect_true(small_predictions < 0.01)\n  expect_true(large_predictions < 0.01)\n})\n\n\n\nHow does the refit model look?\nRefitting models automatically in production is a great way of continuing to learn from new data, but you lose the human oversight of the model which can lead to over-fitting. In addition to the tests listed above you can do some basic checks on the model itself to make sure that it’s being generated properly.\n\nlibrary(broom)\nnew_model <- lm(SalePrice ~ LotArea + Neighborhood, new_data)\n\n#Did this actually generate a model?\nexpect_is(new_model, \"lm\")\nexpect_true(glance(new_model)$r.squared > 0.5)\n\n# Check outliers among the Neighborhood dummy variables\nmax_neighborhood_beta <- new_model %>% \n  tidy() %>%  \n  filter(str_detect(term, \"Neighborhood\")) %>% \n  pull(estimate) %>% \n  max(abs(.))\nexpect_true(max_neighborhood_beta < 2e5)\n\nAgain you can substitute whatever you want for measures of model performance. The important thing is that you clarify what you are expecting and have your computer check that those expectations are met every time the model is re-fit.\n\n\nIs it making illegal or unethical decisions?\nTesting is a great way to check whether your model is making distinctions that it shouldn’t. I’m a lawyer by training so I’m aware of some of the problems companies can run into when they start making illegal distinctions between customers. For instance, if you implement a model that sentences people differently based on their race or gender, your company runs the risk of legal sanction. These sanctions can be ruinous for a company, so it’s definitely worth writing a test about.\nA good way of evaluating whether your model is making these distinctions is to sample from various ethnic or gender groups in your data, run those samples through your data pipeline, and check that the predictions are within the same range. If they are not the same, it’s worth worrying about whether your model is discriminatory. In Canada there’s something called “adverse effect discrimination,” which means you can be liable for a model that discriminates against a particular group even if you don’t include information about group membership in your model. If you consult with a lawyer about what kind of distinctions are allowed, you can then build tests to ensure that you and future data scientists are not stepping over those lines.\nTo show how this test would work, let’s generate racial categories for this data.\n\nhouse$seller_race <- sample(c(\"Aboriginal\", \"Black\", \"White\"), nrow(house), TRUE)\n\nsub_groups <- house %>% \n  group_by(seller_race) %>% \n  nest() %>% \n  mutate(preds = map(data, ~predict(model, .))) %>% \n  mutate(mean_predicted = round(map_dbl(preds, mean))) %>% \n  select(seller_race, mean_predicted) %>% \n  spread(seller_race, mean_predicted)\n\ntest_that(\"No significant difference between racial groups\", {\n  expect_false(sub_groups$Aboriginal[1] < sub_groups$White[1])\n  expect_false(sub_groups$Black[1] < sub_groups$White[1])\n})\n\nMy guess is that a large proportion of data science models would fail checks like this because so many datasets on which those models are based are biased in one way other another. The result is that even if the model doesn’t explicitly rely on racial or gender by including those features in the model, they might be systematically biased against minotiry groups. This should make everybody worried as these models get embedded into more aspects of our lives. For instance a model which generates home loan interest rates might charge minority groups higher rates, or a sentencing model might prevent minority groups from qualifying for bail. These models are likely illegal, and certainly unethical so it’s worth creating tests which check that you are not accidentally making those kinds of distinctions."
  },
  {
    "objectID": "posts/2017-02-02-r-for-excel-users/index.html",
    "href": "posts/2017-02-02-r-for-excel-users/index.html",
    "title": "“R for Excel Users”",
    "section": "",
    "text": "Like most people, I first learned to work with numbers through an Excel spreadsheet. After graduating with an undergraduate philosophy degree, I somehow convinced a medical device marketing firm to give me a job writing Excel reports on the orthopedic biomaterials market. When I first started, I remember not knowing how to anything, but after a few months I became fairly proficient with the tool, and was able to build all sorts of useful models. When you think about it, this is an amazing feature of Excel. Every day, all over the world, people open up a spreadsheet to do some data entry and then, bit by bit, learn to do increasingly complex analytical tasks. Excel is a master at teaching people how to use Excel.\nR is not like that. I learned to use R as a side project during law school, and it felt a bit like training with an abusive kung-fu master in the mountains of rural China.\nI couldn’t get R to do anything. It wouldn’t read in files, draw a plot or multiply two numbers together. All I could do was generate mystifying errors and get mocked on Stack Overflow for asking redundant questions. This was all made more frustrating by the fact that I could accomplish all of these things in Excel without much difficulty.\nThis is the basic pain of learning to program. Programming languages are designed to be general in their application and to allow you to accomplish a huge variety of complex tasks with the same basic set of tools. The cost of this generality is a steep learning curve. When you start learning to do basic tasks in R, you are also learning how to do complex things down the road. As you learn more and more, the marginal cost of complex analyses goes down. Excel is the opposite, and is very easy at the beginning, but the marginal cost goes up with the complexity of the problem. If you were to graph this it might look like this:\nAt the beginning, when you are trying to accomplish simple things like balancing a budget or entering some data by hand, R is definitely harder to learn than Excel. However, as the task gets more complex, it becomes easier to accomplish in R than Excel, because the core structures of Excel are designed for relatively simple use cases and are not the best for more complex problems. This isn’t to say that you can’t solve a lot of complex problems with Excel, it’s just that the tool won’t make it easy for you.\nFor a lot of us, the pain of learning to program feels like the pain of failure. When the program gives you an incomprehensible error message it feels like it’s telling you that you’re stupid and lack programming aptitude. But after programming for a while, you learn that nobody really understands those errors, and everybody feels like an imposter when their program fails. The pain you feel is not the pain of failure, it’s just the pain of learning."
  },
  {
    "objectID": "posts/2017-02-02-r-for-excel-users/index.html#why-is-learning-new-things-so-hard",
    "href": "posts/2017-02-02-r-for-excel-users/index.html#why-is-learning-new-things-so-hard",
    "title": "“R for Excel Users”",
    "section": "Why is learning new things so hard?!",
    "text": "Why is learning new things so hard?!\nThe difficulty of learning a new tool is caused by two obstacles:\n\nObstacle #1: The tool is different from what you know\nWhen you know how to use something you have this vast amount of basic vocabulary about that tool. I haven’t used Excel seriously for six years, but I can still remember all of its hot-keys, formula names, and menu structures. When you’re learning a new tool you don’t know any of this stuff, and that automatically makes it more difficult. Additionally, you might know where to look to find help on the old tool, or how to Google questions in such a way that you find useful answers. You don’t know any of these things about the new tool, which is painful.\n\n\nObstacle #2: The mental model underlying the tool is different from your current mental model\nThe way the new tool wants you to think about the problem is different from the way you are used to thinking about the problem. For instance, if you are used to putting your analysis in a rectangular grid, then moving to a tool which is designed around procedural commands is going to be difficult.\nIn my opinion obstacle #2 is by far the larger barrier for Excel users. Most of the people who learn R have some basis in programming. The mental models underlying languages like Matlab or Python, as well as statistical packages like SPSS and SAS, have a lot in common with R, and there are many resources available for translating the bits which don’t make sense. Excel makes you think about analytical problems in a very different way, and there aren’t very many resources for translating the two paradigms."
  },
  {
    "objectID": "posts/2017-02-02-r-for-excel-users/index.html#four-fundamental-differences-between-r-and-excel",
    "href": "posts/2017-02-02-r-for-excel-users/index.html#four-fundamental-differences-between-r-and-excel",
    "title": "“R for Excel Users”",
    "section": "Four Fundamental Differences Between R and Excel",
    "text": "Four Fundamental Differences Between R and Excel\n\n1) Text-based analysis\nExcel is based on the physical spreadsheet, or accountant’s ledger. This was a large piece of paper with rows and columns. Records were stored in the first column on the left, calculations on those records were stored in the boxes to the right, and the sum of those calculations was totaled at the bottom. I would call this a referential model of computation which has a few qualities:\n\nThe data and computation are usually stored in the same place\nData is identified by its location on the grid. Usually you don’t name a data range in Excel, but instead refer to it by its location, for instance with $A1:C$36\nThe calculations are usually the same shape as the data. In other words if you want to multiply 20 numbers stored in cells A1:An by 2, you will need 20 calculations: =A1 * 2, =A2 * 2, ...., =An * 2.\n\nText based data analysis is different:\n\nData and computation are separate. You have one file which stores the data and another file which stores the commands which tell the program how to manipulate that data. This leads to a procedural kind of model in which the raw data is fed through a set of instructions and the output pops out the other side.\nData is generally referenced by name. Instead of having a dataset which lives in the range of $A1:C$36 you name the data set when you read it in, and refer to it by that name whenever you want to do something with it. You can do this with Excel by naming ranges of cells, but most people don’t do this.\n\n\n\n2) Data structures\nExcel has only one basic data structure: the cell. Cells are extremely flexible in that they can store numeric, character, logical or formula information. The cost of this flexibility is unpredictability. For instance you can store the character “6” in a cell when you mean to store the number 6.\nThe basic R data structure is a vector. You can think of a vector like a column in an Excel spreadsheet with the limitation that all the data in that vector must be of the same type. If it is a character vector, every element must be a character; if it is a logical vector, every element must be TRUE or FALSE; if it’s numeric you can trust that every element is a number. There’s no such constraint in Excel: you might have a column which has a bunch of numbers, but then some explanatory test intermingled with the numbers. This isn’t allowed in R.\n\n\n3) Iteration\nIteration is one of the most powerful features of programming languages and is a big adjustment for Excel users. Iteration is just getting the computer to do the same thing over and over again for some period of time. Maybe you want to draw the same graph based on fifty different data sets, or read and filter a lot of data tables. In a programming language like R you write a script which works for all of the cases which you want to apply it to, and then tell the computer to do the application.\nExcel analysts typically do a lot of this iteration themselves. For instance if an Excel analyst wanted to combine ten different .xls files into one big file, they would probably open each one individually, copy the data, and paste it into a master spreadsheet. The analyst is effectively taking the place of a for loop by doing one thing over and over again until a condition is met.\n\n\n4) Simplification through abstraction\nAnother major difference is that programming encourages you to simplify your analysis by abstracting common functions from that analysis. In the example above you might find that you have to read in the same type of files over and over again and check that they have the right number of rows. R allows you to write a function which does this:\n\nread_and_check <- function(file){\n  out <- read.csv(file)\n  if(nrow(out) == 0) {\n    stop(\"There's no data in this file!\")\n  } else {\n    out\n  }\n}\n\nAll this function does is read in a .csv file and then check to see if it has more than zero rows. If it doesn’t, it returns an error. Otherwise it returns the file (which is called “out”). This is a powerful approach because it helps you save time and reduce errors. For instance, if you want to check if the file has more than 23 rows, you only have to change the condition in one place rather than in several spreadsheets.\nThere’s really no analog for these kinds of functions in an Excel-based workflow, and when most analysts get to this point they just start writing VBA code to do some of this work.\n\n\nExample: Joining two tables together\nI thought I’d illustrate these principles by working through the example of joining two tables together in Excel and R. Let’s say that we had two data tables, one with some information about cars and another with the colour of those cars, and we want to join the two of them together. For the purpose of this exercise, we’re going to assume that the number of cylinders in a car determines its colour.\n\nlibrary(dplyr)\nlibrary(knitr)\n\nWarning: package 'knitr' was built under R version 4.1.2\n\ncars <- mtcars\ncolours <- data_frame(\n  cyl = unique(cars$cyl),\n  colour = c(\"Blue\", \"Green\", \"Eggplant\")\n)\n\nkable(cars[1:10, ]) #kable is just for displaying the table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nMazda RX4\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\nValiant\n18.1\n6\n225.0\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n\n\nDuster 360\n14.3\n8\n360.0\n245\n3.21\n3.570\n15.84\n0\n0\n3\n4\n\n\nMerc 240D\n24.4\n4\n146.7\n62\n3.69\n3.190\n20.00\n1\n0\n4\n2\n\n\nMerc 230\n22.8\n4\n140.8\n95\n3.92\n3.150\n22.90\n1\n0\n4\n2\n\n\nMerc 280\n19.2\n6\n167.6\n123\n3.92\n3.440\n18.30\n1\n0\n4\n4\n\n\n\n\n\n\nkable(colours)\n\n\n\n\ncyl\ncolour\n\n\n\n\n6\nBlue\n\n\n4\nGreen\n\n\n8\nEggplant\n\n\n\n\n\nIn Excel you would probably do this using the VLOOKUP() function, which takes a key, and a range, and then looks up the value of that key within that range. I put together an example spreadsheet of this approach here. Notice that in each lookup cell I typed some version of =vlookup(C4,$H$2:$I$5, 2, FALSE). This illustrates a few things. First, the calculation is the same shape as the data, and happens in the same file as the data. We have as many formulas as we have things that we want to lookup, and they are placed right next to the dataset. If you’ve used this approach you can probably remember making mistakes in the process of writing and filling this formula. Second, the data is referred to by its address on the sheet. If we move the lookup table to another sheet, or another place on this sheet, that is going to screw up out lookup. Third, notice that the first entry of the cyl column in the spreadsheet store in C2 is stored as text, which causes error in the lookup function. In R, you would have to store all the calendar values as a numeric or character vector.\nTo do the same thing in R, we would use this code:\n\nleft_join(cars, colours, by = \"cyl\") %>% \n  filter(row_number() %in% 1:10) %>% # to display only a subset of the data\n  kable() \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\ncolour\n\n\n\n\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\nBlue\n\n\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\nBlue\n\n\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\nGreen\n\n\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\nBlue\n\n\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\nEggplant\n\n\n18.1\n6\n225.0\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\nBlue\n\n\n14.3\n8\n360.0\n245\n3.21\n3.570\n15.84\n0\n0\n3\n4\nEggplant\n\n\n24.4\n4\n146.7\n62\n3.69\n3.190\n20.00\n1\n0\n4\n2\nGreen\n\n\n22.8\n4\n140.8\n95\n3.92\n3.150\n22.90\n1\n0\n4\n2\nGreen\n\n\n19.2\n6\n167.6\n123\n3.92\n3.440\n18.30\n1\n0\n4\n4\nBlue\n\n\n\n\n\nHere we refer to the data by its name, use one function to operate on the whole table rather than row by row. Because consistency is enforced for each vector we can’t accidentally store a character entry in a numeric vector.\n\n\nIteration\nNow let’s say we wanted to get the mean displacement for each colour of car. Most Excel users would probably do this iteration manually, first selecting the table, sorting it by colour and then picking out the ranges that they wanted to average. A more sophisticated analyst would probably use the averageif() function to pick out the criteria they wanted to average on, and so avoid a few errors. Both approaches are implemented in the iteration tab of the spreadsheet.\nIn R you would do something like this:\n\nleft_join(cars, colours, by = \"cyl\") %>% \n  group_by(colour) %>% \n  summarize(mean_displacement = mean(disp)) %>% \n  kable()\n\n\n\n\ncolour\nmean_displacement\n\n\n\n\nBlue\n183.3143\n\n\nEggplant\n353.1000\n\n\nGreen\n105.1364\n\n\n\n\n\nWhat this does is takes the data set, splits it up by the grouping variable, in this case colour, then applies the function in the summarize function to each group. Again, the difference is that we’re always referring to things by name rather than location, there is one line of code which applies the function to the whole dataset, and all of the iterative actions are stored in the script.\n\n\nGeneralizing through functions\nFunctions are among the more difficult parts of learning to program, and you really can get by for quite a long time without ever learning to use them. I wanted to include them just because they are common and can be quite discouraging for Excel users because they are totally foreign to their workflow. A function is a way of using existing code on new objects. In the case above it might look like this:\n\njoin_and_summarize <- function(df, colour_df){\n  left_join(df, colour_df, by = \"cyl\") %>% \n    group_by(colour) %>% \n    summarize(mean_displacement = mean(disp))\n}\n\nThe things between the function() braces (df and colour_df) are called “arguments”, and when you call the function all it does is take the actual objects you supply to the function and plugs them in to wherever that argument appears between the curly braces. In this case we would plug in cars for the df argument, and colours for the colour_df argument. The function then basically replaces all the dfs with cars and colour_dfs with colours and then evaluates the code.\n\njoin_and_summarize(cars, colours) %>% \n  kable() \n\n\n\n\ncolour\nmean_displacement\n\n\n\n\nBlue\n183.3143\n\n\nEggplant\n353.1000\n\n\nGreen\n105.1364\n\n\n\n\n\n\n\nConclusion\nExcel users have a strong mental model of how data analysis works, and this makes learning to program more difficult. However, learning to program will allow you to do things that you can’t do easily in Excel, and it really is worth the pain of learning the new model."
  },
  {
    "objectID": "posts/2020-06-21-masks-and-vitamin-d/index.html",
    "href": "posts/2020-06-21-masks-and-vitamin-d/index.html",
    "title": "Masks and Vitamin D",
    "section": "",
    "text": "Imagine that someone offered you a free lottery ticket. You would have a small chance of winning a million dollars, but the ticket doesn’t cost anything. It would be silly to turn down this ticket because you thought your odds of winning were either too small or too unclear; the only reason we care about the odds of winning a game is so that we can determine if the expected value of winning is higher than the expected cost of playing. If the ticket is free, then so long as there is any chance you might win, the rational decision is to play.\nDoctors don’t tend to think about prescriptions as probabilistic bets. Most drug therapies are pretty expensive and relatively dangerous, so you only want to recommend them when you’re very sure that they work. Until you have strong evidence to do something, the safest bet is usually to do nothing. This is why most public health agencies in the western hemisphere initially doubted the efficacy of cloth face masks: in the absence of any evidence that they really helped with Covid, they didn’t recommend them. Most of these agencies now recommend that the public wear face masks, even if they have a low probability of meaningfully changing the outbreak. They are like free lottery tickets: it would be fantastic if they helped reduce the spread of the disease, and there’s no real cost if they don’t work.\nVitamin D is another free lottery ticket that, for whatever reason, hasn’t gotten any traction with doctors or public health officials. I see a lot of doctors on Twitter say that the evidence for vitamin D supplementation is extremely weak, and then turn around and tell everyone to wear face masks while jogging. The evidence for vitamin D is weak, but it’s stronger than the evidence for universal masking. While we don’t have randomized trials of either intervention, vitamin D has been shown to treat other respiratory infections, and we at least have some observational evidence that many Covid-19 patients have low vitamin D levels. To be clear, I support both interventions because they’re low-risk, high upside bets, but there’s no consistent way to look at the totality of the information and come to the conclusion that we should wear cloth masks, but not also monitor and treat vitamin D levels.\nSo what is the evidence for the two interventions?"
  },
  {
    "objectID": "posts/2020-06-21-masks-and-vitamin-d/index.html#randomized-controlled-trials",
    "href": "posts/2020-06-21-masks-and-vitamin-d/index.html#randomized-controlled-trials",
    "title": "Masks and Vitamin D",
    "section": "Randomized controlled trials",
    "text": "Randomized controlled trials\nRCT trials are the easiest way to identify causes because they allow researchers to isolate a particular causal mechanism. It’s important to remember that you can do causal inference without a randomized trial, and for many things like smoking or diet, randomized trials are either too difficult to perform or unethical.\nAs far as I’m aware there are no RCT trials evaluating whether universal masking actually reduces ones odds of contracting Covid-19.\nWe do have one small randomized vitamin D trial where researchers gave hospitalized patients calcifediol, a vitamin D metabolite, and evaluated whether their risk of ICU admission went down. The results were dramatic:\n\nOf 50 patients treated with calcifediol, one required admission to the ICU (2%), while of 26 untreated patients, 13 required admission (50 %) p value X2 Fischer test p < 0.001. Univariate Risk Estimate Odds Ratio for ICU in patients with Calcifediol treatment versus without Calcifediol treatment: 0.02 (95 %CI 0.002−0.17).\n\nThis trial is probably too small to evaluate the precise effect size, but it’s very likely that there is a positive causal effect."
  },
  {
    "objectID": "posts/2020-06-21-masks-and-vitamin-d/index.html#prior-belief",
    "href": "posts/2020-06-21-masks-and-vitamin-d/index.html#prior-belief",
    "title": "Masks and Vitamin D",
    "section": "Prior belief",
    "text": "Prior belief\nOne way to figure out how to treat novel viruses is to look at what has worked when treating similar viruses. Covid-19 is a respiratory virus, so it’s reasonable to assume that things that treat or prevent respiratory viruses are good bets to treat or prevent Covid. The flaw in this reasoning is, of course, that Covid is new and so the virus might behave differently in ways that we don’t understand and that undermine the usefulness of prior studies. Most of the data behind cloth masks falls into this category; we have some trials that show that masks prevent influenza and other respiratory ailments, so we should have a prior belief that they will also help with Covid-19. That said, a recent review of this evidence concluded that:\n\nMost included trials had poor design, reporting and sparse events. There was insufficient evidence to provide a recommendation on the use of facial barriers without other measures. We found insufficient evidence for a difference between surgical masks and N95 respirators and limited evidence to support effectiveness of quarantine. Based on observational evidence from the previous SARS epidemic included in the previous version of our Cochrane review we recommend the use of masks combined with other measures.\n\nIn other words, while there’s no strong evidence that universal masking prevents similar illnesses, there is some evidence so why not wear a mask?\nThe evidence that vitamin D prevents respiratory illnesses is stronger because it has been more widely studied. A meta-analysis found two important facts about vitamin D supplementation and other respiratory conditions:\n\nOverall, vitamin D supplementation reduced the incidence of respiratory infection by 20%.\nFor the very deficient, supplementation reduced the incidence by around 70%.\n\nAgain, information about other illnesses might not give us much information about Covid, but at least in the case of vitamin D we have strong evidence that it actually helps those other illnesses."
  },
  {
    "objectID": "posts/2020-06-21-masks-and-vitamin-d/index.html#mechanistic-studies",
    "href": "posts/2020-06-21-masks-and-vitamin-d/index.html#mechanistic-studies",
    "title": "Masks and Vitamin D",
    "section": "Mechanistic studies",
    "text": "Mechanistic studies\nMechanistic studies are where you look at a simplified version of a proposed relationship to establish whether it’s plausible. They’re important because they help debunk correlations which might appear in the data but which couldn’t exist because there’s no realistic causal mechanisms. Mechanistic studies can only show you that a causal relationship is plausible, not that it actually exists. We have a few mechanistic studies of masks and Covid that show that cloth masks can block droplet particles when people speak or cough. What these studies specifically demonstrate is that, when worn in lab conditions, homemade masks block aerosols which we think transmit Covid. So if Covid is transmitted by those particles, and if mask wearing doesn’t increase risk in other ways, it’s plausible that cloth masks could help prevent the spread of the disease. The problem with this type of information is that we don’t know the mechanics of the Covid virus, so it’s possible that the assumptions underlying the mechanistic studies are wrong. For example, it’s possible that the virus is transmitted through droplets that are blocked by the mask, but it’s also possible that it’s transmitted by smaller particles that aren’t blocked by it. Alternately, it’s possible that touching your eyes and nose is the main transmission route and so masks may actually be harmful - we just don’t know.\nThe mechanistic evidence for vitamin D has the same weaknesses. We have some idea that Covid involves the ACE2 receptor, overactive immune response, and problems with blood clotting. These are all processes that involve vitamin D, and are proesses for which supplementation has been shown to be helpful, but we don’t know whether supplementation of Covid-19 patients will actually improve anything specific to that disease. Additionally we know that calcitriol, the active form of Vitamin D, is active against the SARS-COV2 virus in the lab, but we don’t know if that will improve illness in humans. It’s therefore plausible that vitamin D could ameliorate severe Covid, but we don’t know whether it actually does. Overall the mechanistic case for mask wearing is probably stronger than for vitamin D because it’s a much simpler mechanism, but it’s very far from proof that cloth masks prevent infection."
  },
  {
    "objectID": "posts/2020-06-21-masks-and-vitamin-d/index.html#ecological-studies",
    "href": "posts/2020-06-21-masks-and-vitamin-d/index.html#ecological-studies",
    "title": "Masks and Vitamin D",
    "section": "Ecological studies",
    "text": "Ecological studies\nEcological studies look at groups of people and identify relationships between qualities of those groups. For instance, you might look at a set of countries and discover that countries with high sugar consumption also have high cancer rates. The general problem with ecological studies is that if you compare enough data points you can usually find relationships even if they don’t really exist, so it’s easy to sift through a dataset and find a number of spurious correlations.\nFor masks, we have some ecological studies that find that government directives to wear masks coincide with lower Covid cases. There are lots of problems with these studies. For example, most states make mask recommendations after making lots of other recommendations like social distancing and hand washing, and it’s impossible to separate the effects of the various public health interventions. Additionally, mask recommendations typically only occur when there’s an adequate supply of masks for health care workers, so the directive for the public to wear masks may be just a proxy for states that have adequate PPE for their health care workers. Finally, the study doesn’t include any data about whether these directives were enforced or obeyed.\nThe ecological studies for vitamin D have all of the same problems. There’s a relationship between latitude and Covid-19 outcomes that doesn’t go away when you control for age, but there are plenty of potential confounding variables like heat, humidity, or BCG vaccinations. Similarly, the rate of vitamin D deficiency predicts Covid severity, but there are lots of other differences between the countries in the sample. Finally, populations at high risk for Covid like elderly people or people with darker skin also tend to have higher rates of vitamin D deficiency, but there are other factors in those populations that could cause that excess risk [TODO do you want to link to your other post?]."
  },
  {
    "objectID": "posts/2020-06-21-masks-and-vitamin-d/index.html#observational-studies",
    "href": "posts/2020-06-21-masks-and-vitamin-d/index.html#observational-studies",
    "title": "Masks and Vitamin D",
    "section": "Observational studies",
    "text": "Observational studies\nObservational studies are when you look at an existing patient pool and ask whether some characteristics are over- or underrepresented in that pool. The problem with observational studies is that it’s hard to control for confounding variables, so it’s pretty common for there to be a strong relationship in an observational study which disappears when you do a randomized controlled trial.\nThere are no strong observational studies that wearing masks prevents Covid. Nobody has found that mask wearers are underrepresented among Covid patients, or that people who don’t wear masks are overrepresented. There is one study that found that households that reported wearing masks also reported fewer secondary infections from Covid-19, but since you don’t know what percentage of the people who reported wearing masks actually did, it’s hard to make an inference about the efficacy of face masks. People who contracted Covid from a mask-wearing loved one may be less likely to report honestly on their family’s mask-wearing practices than those who didn’t contract the disease, because of social pressure to support the efficacy and uptake of public health recommendations.\nWe do have a few observational studies that vitamin D levels are associated with Covid outcomes:\n\nA Mendelian randomisation analysis of excess Covid-19 mortality of African-Americans in the US suggests that vitamin D is a risk factor for Covid Mortality.\nA re-analysis of 107 Swiss blood samples found that PCA positive patients had 25-hydroxyvitamin D concentrations of half that of PCA negatives. This finding held after stratifying for age and gender.\nHospitalized male Covid-19 patients were found to have lower vitamin D levels than controls.\nA retrospective cohort study found that Chicago patients who were likely vitamin D deficient were more likely to test positive for Covid-19.\nA observational study from Belgium found that vitamin D deficiency is correlated with the risk for hospitalization for Covid-19 pneumonia and predisposes patients to more advanced radiological disease stages.\nA small cohort trial found that just 16% of patients who received vitamin D, magnesium, and vitamin B12 required oxygen, compared with 61.5% of the previous cohort who did not receive DBM supplementation.\nA retrospective study in the Philippines found a significant association between vitamin D status and severe Covid-19 infections.\n\nTo be clear, all of these studies have problems, but flawed observational trials are still a heck of a lot better than nothing. We have at least observed vitamin D levels in people with Covid-19 and found that those levels tend to be lower in more severe cases. We do not have that type of information for homemade masks."
  },
  {
    "objectID": "posts/2022-01-11-ms-covid/index.html",
    "href": "posts/2022-01-11-ms-covid/index.html",
    "title": "What MS taught me about the pandemic",
    "section": "",
    "text": "By all rights I should’ve been terrified when the Covid-19 pandemic started. After all, I’m one of those vulnerable, immune-suppressed people that we’re all supposed to shield from illness, but over the last two year’s, I’ve found that I’ve had a more stable emotional experience than most of the people that I’ve interacted with.\nPandemics are a kind of chronic illness on a societal level. For many people, Covid was the first experience of a health threat that persisted over a long period of time. Dealing with the experience of a chronic illness has taught me some lessons which I’ve found applicable to Covid, and I wanted to share these lessons in case they’re helpful.\n\nYou’re not in control of your health\nWe’re all basically puritans about health. We want to believe that if we live righteously, do all the right things and avoid all of the wrong things, we’ll avoid all the horrors of serious illness. When someone has a heart attack, we ask whether they ate right and exercised, and when someone gets cancer, we think back to the years that they smoked.\nThe reality, though, is that most of the determinants of health are entirely out of our control. You’re born into a particular genetic and social situation and that, along with a whole lot of luck, determines a lot about your health. My biggest risk factor for MS was the fact that I had mononucleosis as a college student in Canada. It turns out that getting mono as an adult while you’re vitamin D deficient radically increases your risk of developing MS later in life, and while there were probably things I could have done to reduce my risk, it was largely set in stone by the time I was twenty-one. Similarly, after diagnosis there are a few things that you can do to improve your outcomes, but aside from taking your medication they’re all pretty marginal interventions. Your individual decisions let you pick which dice to roll, but they don’t determine which numbers come up. Virtuous people still get sick.\nWhat I’ve learned is that given all this, there’s a really narrow spectrum of things that are worth worrying about. If something has a big enough effect to change my decisions, and I’m fairly certain that that effect is real, I will look into it, but otherwise I don’t pay attention to it. This means that I ignore all studies on animal models (not certain enough) and all papers about how MS patients do over the course of their lives (doesn’t change my decisions). This is important because worrying about illness costs me something. It increases stress, damages my relationships, and reduces my happiness, and over the course of a lifetime these costs really add up. Limiting what you worry about to the things that actually change your decisions is a vital self-care strategy which helps to keep your background stress level in check.\nThis is an important lesson as we move into the endemic phase of Covid. Whenever there’s a new Covid variant there’s a period of several months where we don’t really know anything about the variant, but everyone is still freaking out about it. All of the information at that point falls outside of the constructive worry spectrum. If you don’t really know whether a variant is different enough to change your decisions, there’s no point in really worrying about it. As a result, the resilient thing to do is to just not worry about it and trust that you can come back to the news in a month or two and not really miss anything.\n\n\nThings are never going back to how they were\nWhen you get a chronic illness, there’s a long period where you deeply want things to go back to how they were before you had it. Being diagnosed with a serious illness is so abrupt and traumatic and it’s hard to even understand what happened. You suddenly move from being a healthy person with a normal future to something less certain and more limited. It’s natural to want to snap your fingers and go back to the person you were before you got sick.\nThe reality though is that even if you’re entirely cured, things are never going to be the same as they were. My own illness has gone just about as well as it could have, I haven’t had any major relapses and overall I’m probably a healthier, happier person than I was before I got MS. Even so there are a thousand yets and so fars that hang over every moment of my life. I haven’t had any major relapses yet. I’m a healthy person so far. Even in the miraculous case that MS is cured in my lifetime, the process of getting sick is one of the two or three more formative experiences in my life. There’s no way I can undo the past six years.\nAbandoning the fantasy that things are going to go back to however they were before is the first step of figuring out how to live with an illness. When you do this, it becomes easier to see that while your life is certainly different, and while it may be objectively worse by some measures, it has just as much value as it did before. You can learn to thrive in the reality you inhabit, but first you have to admit that it’s reality.\nThe Covid-19 pandemic is going to end, but that doesn’t mean that Covid is going to go away, or that your life is going to go back to what it was like in 2019. Many governments have shopped the idea that “vaccines will give you your life back” as a way of convincing people to get vaccinated, and while it’s true that vaccines prevent severe disease and death remarkably well, they will not magically make Covid disappear. Covid is present in dozens of animal reservoirs and much of the world remains unvaccinated. As a result, we should expect regular introductions of new Covid variants which may avoid preexisting immunity. Some of these will be mild, but some of them are going to cause severe disease and loss of life.\nThe rest of your life is going to be lived in a world that experiences periodic Covid outbreaks, and you’re going to have to figure out a way to live in that world. What this looks like is going to be different for different people, but the fundamentals of the pandemic are not going to change all that much going forward, so it’s important to figure out what it looks like to you.\n\n\nDon’t let the illness take your life\nOne of the most dangerous parts of chronic illness is the way that it can become your identity. The illness generates so much uncertainty and fear that it’s tempting to start living your life as though the worst-case scenario has already happened. When I was buying a house I genuinely wondered if I should try to find one without staircases because I was kind of bracing for future mobility problems. Maybe if I didn’t have stairs in my house, it would hurt less if I lost the ability to use them. It’s easy to let an illness destroy your life before it has to. Your goals and aspirations contract, and you give up things that you haven’t lost yet.\nThis is a big mistake for two main reasons. First, it won’t make the future less painful. Losing my ability to walk would hurt just as much whether or not there were stairs in my house, and so any energy I spend preparing for that future is a total waste. Since it’s impossible to know what’s going to happen down the road, it’s better to live a full and enjoyable life and trust that you’re going to be able to meet whatever challenges arise. Similarly, in the pandemic context, I might get Covid, I might go to the hospital or develop a second debilitating post-viral illness. Aside from vaccination, there’s not that much that I can do to avoid or prepare for those outcomes, so it doesn’t make sense to try to plan for them.\nThe second problem with this approach is that it’s extremely bad for your health. Since most MS symptoms are caused by damage to your nervous system, they’re often modified by one’s mental outlook. For example, fatigue, one of the most common MS symptoms, is improved by cognitive behaviour therapy. This doesn’t mean that MS fatigue is wholly psychosomatic, but it does mean that one’s mental outlook plays a role.\nI had pretty persistent numbness for about three years, and for a while I would mentally note how bad it was on any particular day. What I didn’t realize was that every time I paid attention to the physical sensation of numbness I was actually reinforcing the mental pathways which produced it. Even though the cause of the numbness was a three centimeter lesion on my spinal cord, the experience of the numbness was modified by what I paid attention to. When I stopped thinking about the illness, my symptoms improved and I was able to do things like exercise, which, over time, led to the numbness going away.\nFor the past few years I’ve been mostly asymptomatic for MS, but even now if I spend a lot of time thinking about numbness, I’ll begin to feel it. A positive state of mind helps these symptoms both by starting to modify those mental pathways, and also by encouraging good health habits. If you think you have a healthy future you’re more likely to do things like exercise vigorously or take a challenging job which in turn helps create that future.\nMany people have spent two years in daily anxiety about Covid. This is not anyone’s fault, but it’s also not healthy. For example, when omicron was first discovered in South Africa, a lot of my friends immediately started bracing for the worst-case scenario. Even though they were vaccinated, they started doing less and worrying more. Looking back on it, they probably aren’t any better prepared now than they would’ve been if they had spent less time thinking about the variant. All that they succeeded in doing was adding a lot of stress to their lives. Similarly, living in fear of contracting Covid probably makes your Covid outcomes worse. Stress and anxiety suppresses immune function, and can prevent people from getting the rest that they need to fully recover.\nThis mental switch is a difficult, ongoing process. If Covid or MS is in your life every day, it’s hard not to obsess over the downside. Every time I get an MRI, or need to plan for a major life event, I go through a period of downside panic. But learning to adjust where your mind is dwelling helps to identify when this is happening, and to at least ask whether or not it’s helpful.\n\n\nExperience is the basis of compassion\nI grew up Buddhist, and one of the main challenges that Buddhists face is trying to develop a kind of intuitive compassion for other people. This can be challenging if, like me, you haven’t experienced all that much misfortune in your life. It’s hard to relate with people who are dealing with serious problems if you don’t really know what it’s like to experience similar problems. Without that intuition, all you have is a kind of paper-mache compassion – it looks a bit like the real thing, but it’s weightless.\nI remember the very first feeling I had after being diagnosed with MS was an odd kind of gratitude. In that moment I could really understand how hard it is to be a young doctor delivering bad news to a patient; how hard it is to receive that news; and how hard it is to live with the fear and uncertainty that comes from it. While this wasn’t how I felt most of the time over the first few years, whenever I was feeling a lot of fear or despair I tried to tune into that feeling of understanding. I know what it’s like to get a spinal tap. I know what it’s like to not be able to type. I know what it’s like to be denied insurance. While these experiences were awful, each of them were another step on the path of understanding what other people are going through.\nLiving with MS has, more than anything, helped me support and understand the people around me. Last spring one of my oldest friends was diagnosed with and later died from skin cancer. Early in her illness, we had a conversation about how to handle the diagnostic process. How lonely it can be, and how alienating other people’s reactions can be. Because I had lived through something sort of similar, I was able to help in a way that I probably wouldn’t have been able to do before having an illness like MS.\nCovid has provided a lot of these experiences. That feeling of dread you have whenever someone coughs nearby is how people with cystic fibrosis have felt for their whole lives. That feeling of being trapped in your apartment with nowhere safe to go is shared by millions of people with disabilities. That anger you feel at the unvaccinated is what immunocompromised people feel when they learn that their friends didn’t get the flu shot this year. These were all bad experiences, but they can still be useful. Maybe we’ll do a better job of supporting health care workers and vulnerable people in society. Maybe more people will get a flu shot next year. None of us get to choose the bad things that happen in our lives, but we do get to decide what we take from them."
  },
  {
    "objectID": "posts/2020-10-23-taking-vitamin-d-back-from-the-racists/index.html",
    "href": "posts/2020-10-23-taking-vitamin-d-back-from-the-racists/index.html",
    "title": "Taking vitamin D back from the racists",
    "section": "",
    "text": "Black and brown people in northern countries have been disproportionately affected by Covid-19. In the US, Sweden, Canada, and the UK, racialized people have been more likely to contract the disease, more likely to have severe courses, and more likely to die from it. The explanation you usually get for this is that excess mortality is caused by systemic racism or social determinants of health. Under this explanation, there’s nothing that surprising about the high Covid mortality because it’s just another example of discriminatory health care policies. This explanation is too vague.\nBoth systemic racism and social determinants of health encompass a huge variety of specific social inequities, and dismantling those systems of oppression is accomplished by addressing the specific inequities more or less one at a time. Even large pieces of legislation, like the 1964 Civil Rights Act, or the Canadian Charter of Rights and Freedoms, were really just attempts to address a small portion of the problem of systemic racism. I think it’s important to talk about the specifics of racial health inequality for two main reasons.\nFirstly, specific causes of racial health inequality are testable. There aren’t any countries in the world that are free from systemic racism, so it’s hard to find motivating examples of how to eliminate it completely. This matters because public policy is extremely complicated, and without real world examples, governments can often cause more harm than good. To make a change, and measure whether that change worked, we need to have a specific causal mechanism in mind.\nSecondly, getting specific about social determinants of health holds people to account. White progressives like myself are often in favour of racial equality right up until the point that it costs us something. We’re in favour of school integration until we have to send our kids to school, we support housing justice until someone tries to build low-income housing in our neighborhood, and we support defunding the police unless there’s a slight increase in property crime. One of the strategies we use to resolve this cognitive dissonance is compare our small contributions to the to big, vague, intractable causes of racial inequality. We can justify sending our kids to private school because that seems like a tiny drop in the systemic racism bucket. Focusing on specific causes is a great way to hold people to account. If you know that housing discrimination is a major problem that can be ameliorated by better zoning laws, it’s worth fixing, even if it’s just a small part of the bigger systemic racism story.\nPeople are often vague when speaking about the social determinants of health in the context of Covid-19 mortality. This is unfortunate because the more specific you are about these causes, the stranger and more concerning it becomes. A recent paper from Sweden highlights some of these puzzles."
  },
  {
    "objectID": "posts/2020-10-23-taking-vitamin-d-back-from-the-racists/index.html#the-swedish-puzzle",
    "href": "posts/2020-10-23-taking-vitamin-d-back-from-the-racists/index.html#the-swedish-puzzle",
    "title": "Taking vitamin D back from the racists",
    "section": "The Swedish Puzzle",
    "text": "The Swedish Puzzle\nSweden has an exceptional social science dataset. They have high quality information about income, housing, and country of origin, and are able to link that dataset with health outcomes on a national level. The researchers took this data and fit two models, one that predicted all cause mortality, and one that predicted Covid-19 mortality. These models controlled for education, net income, marital status and country of birth and found that people who were born in the Middle East or North Africa had a 2-3 fold greater risk of Covid-19 mortality than people born in Sweden. Importantly, they did not find that these groups had a higher all-cause mortality, which indicates that whatever is causing excess mortality in Sweden is different from the determinants of general health, and is not captured by income or education level.\n\nThe second part of the puzzle is that many countries in Africa have extremely low Covid-19 mortality. For example, a Kenyan seroprevalence study estimated that just one in 16,000 infected people were dying from the disease. Another from Ethiopia found that more than 7% of people in Addis Ababa had Covid antibodies at a time when only five people had died from the disease. There are probably many factors that contribute to these numbers but it seems clear that Covid is circulating in Africa, and our best estimate is that not many people are dying from the disease.\nIn Sweden, about six in every 1,000 people who contract Covid die from it, which is several orders of magnitude higher than Kenya or Ethiopia, and remember that Kenyans and Ethiopians in Sweden are dying at a rate three times higher than that, so the question is: What are the determinants of health which are much, much worse for Africans living in Sweden than Africans living in Africa?\nI’ve heard a bunch of explanations for this trend, for example:\n\nSweden is more racist than Ethiopia\nAfrican countries have excellent pandemic responses\nAfrican countries have fewer old people\n\nWhen I try to think through the details of these explanations, none of them really make sense to me. Sure, Sweden is a racist country, but 700,000 people were displaced by ethnic conflict in Ethiopia in 2019, is the effect of that Sweden’s racism really worse? Similarly there’s a lot to admire about the coronavirus response in Africa, but most of those measures should prevent infection, not death, so why is the infection fatality rate low?\nSimilarly, age is an appealing explanation but when you work through the math it doesn’t add up. Let’s make the assumption that Covid didn’t affect anyone in Ethiopia under the age of 65, and that all the cases took place in the capital of Addis Ababa. There are 4.8 million people in the city and about 150,000 of them are over sixty five. If 7% of them had Covid at some point, it would imply that there were 11,000 Covid cases in elderly people. Even with these generous assumptions, this leads to an infection fatality rate of 0.45% which is a three times lower than the 1.4% IFR for people over sixty in rich countries.\nWhen you start thinking specifically about the social determinants of health in Sweden, they make sense locally but not globally. It’s definitely true that immigrants from Africa are discriminated against in Sweden. They have fewer resources, are less likely to be hired for jobs, and receive worse medical care, but they still have access to the health care system of a rich social democracy. Africa, of course, includes some of the poorest countries in the world, and have historically been extremely vulnerable to infectious disease pandemics like HIV. In Ethiopia, 33 million people lack access to running water, 690,000 have HIV, and there are just three doctors per 100,000 people. Many of the things which we think are absolutely critical to reducing the pandemic’s death toll are just not possible in a country with Ethiopia’s resources and infrastructure. People living in extreme poverty are often unable to do things like social distance or wash their hands twenty times a day. If they are able to access a hospital they probably won’t be receiving expensive therapies. So why are they surviving this disease?\nThe rest of this post is going to propose an answer to this question, but I think the question is much more important than my particular answer. If Covid is a disease of poverty, why is it sparing the poorest countries in the world?"
  },
  {
    "objectID": "posts/2020-10-23-taking-vitamin-d-back-from-the-racists/index.html#the-rickets-pattern",
    "href": "posts/2020-10-23-taking-vitamin-d-back-from-the-racists/index.html#the-rickets-pattern",
    "title": "Taking vitamin D back from the racists",
    "section": "The Rickets Pattern",
    "text": "The Rickets Pattern\nThis isn’t the first disease that has exhibited this confusing ethnogeographic pattern. At the turn of the last century one of the biggest threats to Black children was rickets. Rickets is a bone disease caused by vitamin D availability, and it was a major contributor to the extremely high infant mortality rates in Black communities. Rickets was thought to be caused by deprivation and poverty, because it was most common among poor populations, especially among Black and Italian communities. However, when comprehensive x-ray surveys were done, researchers discovered that rickets was basically undetectable in places like Puerto Rico. Just like Covid, rickets was a disease of poverty which spared poor countries.\nThe treatment for rickets had actually already been discovered in 1917 by Hess and Unger. This is from an excellent masters thesis by Alison Awardy quoting the original study:\n\nThe results of this community-based study were outstanding. Hess and Unger demonstrated a dramatic improvement in rickets incidence by giving cod liver oil to 80 black infants prophylactically, feeding the children cod liver oil three times daily. More than four-fifths of the infants who took the oil for six months never developed rickets, while 15 of the 16 infants who did not take the oil showed some signs of rickets, “though all of them lived under the same conditions and many in the very same families.”\n\nCod liver oil is, of course, an excellent source of vitamin D, and vitamin D deficiency was the cause all along. People with dark skin in Northern countries were less efficient at manufacturing vitamin D from the sun, and so needed to take a nutritional supplement to make up the difference. In sunny territories like Puerto Rico, vitamin D deficiency was less common and therefore rickets was less of an issue.\nVitamin D deficiency is also a specific social determinant of health which fits the Swedish puzzle quite well. A Somali person who immigrated to Sweden is probably fairly similar to one who stayed in Somalia, but their risk of vitamin D deficiency is much higher because the sun is less intense in Sweden for most of the year.\nThe second reason to think that vitamin D is the cause of this ethnogeographic pattern is that we have a fair amount of independent evidence that vitamin D deficiency worsens Covid outcomes. There have now been 18 observational studies that have shown that vitamin D deficiency worsens Covid disease. Additionally a randomized clinical trial found that vitamin D supplementation virtually eliminated ICU admissions among elderly Covid patients. That kind of effect would go a long way to explaining why Covid has spared many parts of Africa. People can manufacture 10,000 - 20,000 IU by sitting in the intense sun for half an hour, so moving from a Swedish winter to Ethiopia is similar to taking a large vitamin D supplement. If that supplement reduces number of patients who require ICU care, that could be one of the reasons why the IFR in Ethiopia is so much lower than Sweden, and why North Africans in Sweden are so much more likely to die than white Swedes.\nJust to be doubly clear here I am not proposing that vitamin D deficiency is the only explanation for these differences. The evidence is incontrovertible that things like pre-existing conditions, poverty, and access to medical care are major contributors to excess mortality among racial and ethnic minorities in wealthy countries. I am proposing that vitamin D deficiency is a contributing factor, and that it does a lot to explain why sunny countries with lots of poverty, pre-existing conditions, and lack of medical care are weathering the pandemic fairly well."
  },
  {
    "objectID": "posts/2020-10-23-taking-vitamin-d-back-from-the-racists/index.html#reclaiming-vitamin-d-from-the-racists",
    "href": "posts/2020-10-23-taking-vitamin-d-back-from-the-racists/index.html#reclaiming-vitamin-d-from-the-racists",
    "title": "Taking vitamin D back from the racists",
    "section": "Reclaiming vitamin D from the racists",
    "text": "Reclaiming vitamin D from the racists\nOne of the things I’ve noticed while advocating for vitamin D on Twitter is that racists love vitamin D. This makes a lot of sense when you think about the history of American racism. The contradiction at the heart of the American story is that anyone can rise to the level that their talent and industry can take them unless they are prevented from doing so by their skin colour. One of the main ways to resolve this contradiction is to make race biologically relevant. If there were a scientific explanation as to why one race was oppressed, then the existence of slavery and discrimination wouldn’t undermine the American ideal. America still gets to be the land of the free, it’s just that that freedom is bound by biology.\nVitamin D fits right into this project because it’s one of the very few biological systems in which skin colour is relevant. If your skin is more melanated, it is more protected against UV damage, but also less effective at manufacturing vitamin D. This is appealing for racists because it allows them to blame racial health disparities on the biological importance of skin colour instead of social factors. If vitamin D causes Covid mortality, then society doesn’t need to do anything to resolve high death rates in Black communities because vitamin D deficiency is not caused by social factors.\nOne side effect of this is that progressive people have largely ceded the vitamin D corner to racists. It’s very difficult to argue that vitamin D is an important factor in progressive circles because people assume that if you think that skin colour is biologically relevant in some circumstances it should be politically relevant in all circumstances. As a result, most progressive health practitioners are very skeptical of vitamin D’s importance. This is unfortunate because vitamin D deficiency is actually a form of medical racism.\nThere are basically two types of discriminatory government policies: direct discrimination where a society makes unjust distinctions between protected classes; and adverse-effect discrimination where they do not reasonably accommodate differences between protected classes. A classic example of adverse-effect discrimination is failing to provide accessible entrances to a building. You’re not directly discriminating against wheelchair users because everyone is technically allowed to use the stairs, but the effect of that conduct is that people who have trouble with stairs are barred from the building.\nPeople have different vitamin D requirements based on their skin tone. A very fair skinned person might only need 15 minutes of sunshine to manufacture enough vitamin D for the day, but someone with dark skin may need much longer than that. As a result, people are going to need to consume different amounts of vitamin D through their food to reach the same serum levels. Despite this most governments have a single RDA regardless of skin tone. They say something like, “Most people need 15 minutes of sunshine during the summer to make enough vitamin D,” and when they say “people” they tend to mean white people. Since people with dark skin tend to need higher supplementation levels than people with light skin to maintain the same serum vitamin D, a single RDA for everyone is adverse-effect discrimination.\nSo when I argue that vitamin D is an important factor, I’m not saying that the other social determinants of health don’t matter. I’m saying that vitamin D deficiency is a social determinant of health that matters too."
  },
  {
    "objectID": "posts/2020-10-23-taking-vitamin-d-back-from-the-racists/index.html#do-black-people-actually-need-vitamin-d",
    "href": "posts/2020-10-23-taking-vitamin-d-back-from-the-racists/index.html#do-black-people-actually-need-vitamin-d",
    "title": "Taking vitamin D back from the racists",
    "section": "Do Black people actually need vitamin D?",
    "text": "Do Black people actually need vitamin D?\nI wanted to conclude by discussing a study which I get sent every time I write about this topic. The study claims that Black people have different amounts of vitamin D binding protein and so even though their serum levels of vitamin D are lower, the amount of “bioavailable” vitamin D is the same. People have read this paper to mean that Black people just don’t need the same amount of vitamin D as white people to maintain good health, and have strongly suggested that people who think vitamin D deficiency is a big health problem for Black communities are being racist.\nI think this reading is problematic for a few reasons.\nFirst, the study uses race as a proxy for genes, and we should never ever do that. Race is a social category, not a genetic one, and so we have no reason to think that a small group of Black people in Baltimore is genetically representative of Black people generally. Indeed there may not be any population anywhere that’s genetically representative of Black people generally. An American descendant of an enslaved person may have ancestors from dozens of African and European countries, and so there’s no reason at all to think that they have that much in common genetically with a recent immigrant from Ethiopia. Despite being assigned to the same racial category, their genetic risk factors may be completely different. If you want to explain something based on genetics, your study has to include actual genetic data. Race is not a good proxy.\nSecond, the conclusions of the study were contradicted by a later paper which found that when vitamin D binding protein was measured using a more appropriate assay, they didn’t see a difference between Black and white people.\n\nThe problem is that the monoclonal-antibody assay gives lower values in blacks who have primarily the Gc1F variant of vitamin D– binding protein. The monoclonal antibody discriminates between the Gc1F and Gc1S variants to provide artificially low values for total vitamin D–binding protein in blacks, thus bringing the conclusion of this study into question.\n\nThird, this is an orthopedic study and doesn’t tell us anything about vitamin D’s effect on lung health. The reason they were able to conclude that vitamin D deficiency wasn’t a big deal in their population is because there wasn’t a difference in bone health. This matters because there’s no reason to think that vitamin D’s mechanism of action is the same for all biological systems. It’s possible that bound vitamin D is unavailable in orthopedic contexts, but bioavailable in respiratory contexts.\nSo overall I think it’s a mistake to conclude that Black people have some kind of fundamentally different vitamin D physiology due to an unarticulated genetic difference."
  },
  {
    "objectID": "posts/2019-04-19-technical-debt-for-data-scientists/index.html",
    "href": "posts/2019-04-19-technical-debt-for-data-scientists/index.html",
    "title": "Technical debt for data scientists",
    "section": "",
    "text": "Technical debt is the process of avoiding work today by promising to do work tomorrow. A team might identify that there’s a small time window for a particular change to be implemented and the only way they can hit that window is to take shortcuts in the development process. They might soberly calculate that the benefits of getting something done now are worth the costs of fixing it later. This kind of technical debt is similar to taking out a mortgage or small business loan. You don’t have the money to realize an opportunity right now, so you borrow that money even though it’s going to cost more down the road. The lifetime cost of the investment goes up, but at least you get to make the investment.\nToo often however, data science technical debt is more like a payday loan. We take shortcuts in developing a solution without an understanding of the risks and costs of those shortcuts, and without a realistic plan for how we’re going to pay back the debt. Code is produced, but it’s not tested, documented, or robust to changes in the system. The result is that data science projects become expensive or impossible to maintain as time goes on.\nHere are a couple of questions to help identify if you have a problem with this kind of technical debt:\nIf you answered “no” to these questions there’s a good chance you are taking on dangerous technical debt. Not only are you trading today’s work for tomorrow, but you can’t know how much future work you’re committing to. The technical debt sits like a time bomb somewhere in your code base, and you don’t know when it will go off or what damage it will cause.\nI think there are four basic areas that data scientists should focus on to improve this part of their work, testing, documentation, robustness, and social interaction."
  },
  {
    "objectID": "posts/2019-04-19-technical-debt-for-data-scientists/index.html#testing",
    "href": "posts/2019-04-19-technical-debt-for-data-scientists/index.html#testing",
    "title": "Technical debt for data scientists",
    "section": "Testing",
    "text": "Testing\nI’m a self-taught data scientist, and during the whole time I was taking online courses, working on sample projects, and working on data analytics at various companies I never once learned how to test code. Data science code starts out simple enough that a single person can hold everything in their head, and so adding a bunch of tests feels like a waste of time. This changed for me when I started working at Crunch on a large open source R package.\nCrunch is a test-first company, and had a policy of exhaustive unit tests for everything that could possibly be tested. To accomplish this goal my boss Neal wrote a whole package which records and plays back http responses to allow you to simulate a web service for tests. Starting work at Crunch was a steep learning curve because it was the first time that I was working on something that was way too complex to hold in my head all at once. I might implement a method for some object that would effect dozens of other methods scattered across thousands of lines of code. The only way to understand what was going on in the system was to read the tests, and trust that those tests were a complete description of how the system was supposed to work. If the tests passed I could be sure that my change was correct even if I couldn’t really envision all the side effects of that change. Bit by painful bit I started to understand the value of this workflow and now think it’s the only way to go.\nThe reality of testing is that the only way to ensure that your code is correct is to test it because tests define correctness. When you write a piece of code, the way you know that that code is correct is that it produces some expected output based on a set of inputs. Usually we run these “tests” in an ad-hoc fashion by running a bit of code and then examining the output, but this doesn’t work as soon as the problem gets even a little complicated or the code runs for a long time. All that you’re doing when you write formal tests is being explicit about what you expect from your solution, and ensuring that those expectations are met every time a change is made to the codebase. This helps other developers understand what the code is supposed to do, and so allows them to change and extend your code down the line. If your team doesn’t write tests then they can’t really communicate what it means for their solution to be correct.\n\nHow to fix this\n\nLook through your code base and ask “do I care if this code is correct” include tests for all such code.\nStart writing smaller, simpler functions. These are easier to test.\nWhen other developers ask you to refactor or extend their code, insist that it includes tests. Do not trust people when they the code is correct. Insist that they define what correctness means.\nMake an organizational policy that tasks aren’t complete until they are tested."
  },
  {
    "objectID": "posts/2019-04-19-technical-debt-for-data-scientists/index.html#documentation",
    "href": "posts/2019-04-19-technical-debt-for-data-scientists/index.html#documentation",
    "title": "Technical debt for data scientists",
    "section": "Documentation",
    "text": "Documentation\nDocumentation is often neglected in the programming world, I think in part because writing prose doesn’t spark joy for many programmers. For instance I’ve often asked “how does this work?” and been told to “RTFS (read the fucking source code). This is a big problem because while reading source code can (sometimes after much pain) tell you what is happening in a system, it can never really tell you why it’s happening, or develop your intuition about what should happen in the future. Source code doesn’t, and shouldn’t, tell a story but that story is important for understanding the problem.\nAs I see it there are a few common things to keep in mind when documenting things: 1) Documentation should have a single purpose: If you are not clear about what the document is supposed to accomplish, you can’t evaluate whether it’s doing a good job. There’s a wonderful article about this topic which is worth a careful read. Every document should have a job and you should be able to say what that job is. 1) Documentation should be written by the expert: Often times, especially with internal documents, we expect the person to update the document as they work through it. In other words we want the person who understands the process the least to document it. This never works because being able to explain a process well requires a nuanced understanding that process which new people do not have. Experts have a better intuition about what’s important and usually can do a better job explaining those concepts than novices. We should treat readers of the document as a user, listen to their struggles, empathize with what they don’t understand, but don’t expect them to fix anything. 1) Documentation should be written by the person who can change the process A lot of times documentation is confusing because the thing you’re trying to explain is confusing. When you’re trying to explain a bit of code, or a process, or some feature of how your organization works you might realize that it really doesn’t make any sense, and rather than documenting that thing you should fix it. This is great advice but it only works if the person writing the documentation has the power to fix it. Therefore the person with that power should be in charge of writing documentation. If they don’t want the responsibility of documenting a process they should delegate the power to change that process.\n\nHow to fix this\n\nTreat documentation as a first-class skill in hiring. Request documentation as part of your programming test, hire people with good examples of technical writing in their background\nAdopt the rule that the person who understands a system best, and has power to change that system is responsible for documenting it\nNever blame the reader, if someone doesn’t understand the documentation, that is a problem with the learning resources\nAdopt a system like Guru to automatically invalidate documents which haven’t been reviewed recently."
  },
  {
    "objectID": "posts/2019-04-19-technical-debt-for-data-scientists/index.html#robust-code",
    "href": "posts/2019-04-19-technical-debt-for-data-scientists/index.html#robust-code",
    "title": "Technical debt for data scientists",
    "section": "Robust code",
    "text": "Robust code\nRobust code, or sturdy code, is code which doesn’t fall down when the situation changes. Usually when we first write a function we do so to solve the problem which is immediately in front of us. The problem with that is that the code can’t be applied to new situations and so if the problem in front of us changes, we need to rewrite the function. There aren’t hard and fast rules which govern what “robustness” is, and there are problems on either side. On the one hand you can have overly specific code which breaks whenever new situations are thrown at it but on the other you can succumb to speculative generality and build complex solution to solve problems that you’ll never face. There are however a couple of good strategies that at least help with the problem:\n\nWrite smaller, more modular functions. Smaller functions tend to be more robust because they do less. If you have one lengthy script that pulls data, fits a model and generates output, then if any part of that process changes the whole thing breaks. By contrast separating that script into multiple smaller functions means that when one part of the pipeline breaks the failure is contained to a small area that’s easy to identify and fix. This is also very helpful for debugging errors because when the system breaks it points you to the small function that failed rather than some unknown part of a larger script.\n\nSeparate pure and impure functions Pure functions are those that only communicate with the world through their inputs and their outputs. Kind of like factories with no windows. The great thing about them is that since you can be sure they aren’t effecting the environment outside of the function you can look at its possible inputs and outputs and know exactly what the function does. Impure functions are necessary for things like saving data, querying a database, or requesting something from an API but they are always less robust because it’s harder to reason about the side effects. Separating the pure and impure parts of a big function means pulling out all the bits can can be purified into their own function and have a second function which does the impure part. For instance you might have one function that prepares a dataset, and a second one that writes that dataset to disk, or a function that generates a SQL query and one that executes it. Instead of one giant impure function, you end up with one complex pure function, and one simple impure one.\nPut up guard rails It’s a good idea to put up guardrails as part of your ongoing development process. So if you only expect an argument to be a character vector then have the function error if it’s not getting that input. There’s an excellent talk by Jenny Bryan about some ways of accomplishing this but the basic point is that you should protect future users from error right from the start of writing your function.\n\nRobustness very much follows from documentation and testing. Usually if a function is difficult to document, then it needs to be refactored or split up to make its purpose clearer. Similarly pure functions are easier to test so ensuring that your code is well tested will push you to split functions up into pure and impure sub functions.\n\nHow to fix this\n\nOrganizationally, allocate time for continual code maintenance and refactoring. Regularly pruning and weeding a code base produces compounding productivity returns and will pay off in the long run\nDo not accept people saying they do not have time to write robust code. Once you get into the habit writing robust code is actually quicker than writing fragile code both in the short run and especially in the long run\nArticulate best practices and point to examples of what you want your code base to look like."
  },
  {
    "objectID": "posts/2019-04-19-technical-debt-for-data-scientists/index.html#social-interaction",
    "href": "posts/2019-04-19-technical-debt-for-data-scientists/index.html#social-interaction",
    "title": "Technical debt for data scientists",
    "section": "Social Interaction",
    "text": "Social Interaction\nIt’s all well and good for an organization to say that it wants to avoid or pay down technical debt, but it’s difficult to get a group of people to do the work to fix things. I think that fundamentally this kind of bad technical debt is more of a social problem than a technical one because people relate to a solution as an individual exercise rather than a contribution to a group project.\nBasically speaking all of the principles in this article are about making it easier for future people to understand, maintain and improve your code. The very best evidence you have about what kinds of problems future people will have with your work are to look at the problems that current people have with it. We all get myopic about the way we solve a particular problem, and so need other people’s feedback to understand conner cases and fix misunderstanding.\nThe best way to solicit this feedback is to try to be egoless about your own solutions and kind about other people’s solutions. The Art of Giving Code Reviews by Alexandra Hill is an excellent piece about this process I recommend that everyone reads it. But the upshot is that talking about code is a conflict-prone area and it’s important to be intentional about improving the trust between members of a team and reducing conflict which arises when you review code. Personally I try to relate with code I write as a draft piece, and think of the code review process as basically sending a first draft to an editor. There’s going to be lots of typos, poor language, and sloppy writing and it’s impossible to resolve those things alone. Similarly when providing feedback I try to remind myself that whatever I’m looking at is the very best work that an intelligent person could do under the circumstances, and to always include compliments alongside areas for improvement. Another excellent practice is to be inquisitive “I’m not following this, can you say more about why you did it this way?” instead of declarative “I can’t understand this, please do it this other way”. This helps avoid conflict as well as fostering an educational opportunity, when someone explains why they did something you might learn that it’s a great way to do it or they may be better able to see the problems with that approach.\n\nHow to fix this\n\nFoster emotional safety on development teams\nCommit to ongoing education and collaboration\nDo code reviews\nUse linters, auto-stylers, and automated checks to automate low value code work"
  },
  {
    "objectID": "posts/2019-04-19-technical-debt-for-data-scientists/index.html#conclusion",
    "href": "posts/2019-04-19-technical-debt-for-data-scientists/index.html#conclusion",
    "title": "Technical debt for data scientists",
    "section": "Conclusion",
    "text": "Conclusion\nThere’s an argument that the technical debt I describe in this article isn’t really debt at all but simply shitty coding practice. While I think that’s true to a large degree, it’s much easier to improve things by saying “let’s pay off some technical debt” than by saying “let’s fix your shitty coding practices”. Whatever you call it, it’s a bad state for your code to be in and you should start fixing it as soon as possible."
  },
  {
    "objectID": "posts/2021-12-02-what-are-blockchains-good-for-anyway.en-us/index.en-us.html",
    "href": "posts/2021-12-02-what-are-blockchains-good-for-anyway.en-us/index.en-us.html",
    "title": "What’s the blockchain good for anyway?",
    "section": "",
    "text": "Most new technologies go through this phase. For example in the late 1990s the internet and e-commerce seemed like a transformative technology, but all of the examples of that technology were kind of silly. Looking back on this era, it’s clear that a revolution was starting, but the way that society changed was pretty different from how people thought it was going to change, and the successfully companies to come our of that era were not the ones most people picked.\nThe new thing that blockchains offer the world is that they have three qualities:\n\nThey can make complicated decisions\nThey do not need a centralized authority\nThe decisions can be made by computers\n\nVarious pieces of technology can do one or two of these things. For example, things like Google and Facebook can make complicated decisions in an automated way, but they’re centralized services. Similarly, Wikipedia, or most financial markets are examples of distributed decision making, but they rely on a lot of human judgment and conversation and are difficult to automate. We also have distributed, automated things like internet protocols, but those protocols can’t really complicated decisions.\nBlockchains really are the first technology that can do all three of these things at the same time, but that comes at a cost. In order to enable distributed automated decision-making, blockchains will use much more code, energy, and compute infrastructure than traditional computer programs. Thinking about blockchains this way makes it easier to see why they might have value. If blockchains are valuable it’s either because they are able to distribute centralized decisions, or because they can automate distributed decisions.\n\nDistribute the automated things\nMost of the blockchain use cases that people know about fall into the category of taking a centralized automated process and distributing it. For example replacing the US Dollar with Bitcoin involves taking an incredibly efficient centralized process and distributing it. A blockchain will only succeed in this area if the value of decentralization exceeds the additional friction which would be introduced by inefficient computation.\nI’m not optimistic about these use cases because most people trust these institutions. The US federal reserve is one of the most robust institutions in human history and unless something significant changes, I don’t see the majority of people accepting inefficient transactions in order to avoid the centralized body. Similarly while it’s possible that a blockchain-based social media protocol may be able to avoid some of the problems with Facebook or Twitter, I don’t think that people care very much about the problems with those companies so I don’t think they will accept a bad user experience in order to avoid using a centralized service.\n\n\nAutomated the distributed things\nBefore blockchains we needed to centralize a process before we could automate it. Before you could start using a machine algorithm or a database to make decisions about something you needed to get everyone to use a centralized system to make those decisions. As a result if something is difficult to centralize it is difficult to automate. For example there’s no centralized ledger of international currency transactions because people don’t trust such a ledger. As a result, sending money around the world involves a lot of individual transactions, intermediaries, and counter party risk. As a result it takes a long time, and costs a lot of money, and isn’t available to everyone. If there were some benevolent deity would faithfully maintain a database of all these transactions we could automate this process, but there’s no such deity so people need to rely on long chains of currency trades to get the job done.\nBlockchains have the potential to revolutionize these processes because it allows people to automate them without the need for a centralized authority. The value proposition is the same as every disruption in the information age: take something that is done by humans and figure out how to get computers to do it. The important thing about these use cases is that the blockchain solution is not competing with a centralized automated solution because centralization isn’t feasible. This means that the inherent inefficiencies of blockchain-based solutions don’t matter that much because they’re still way more efficient than what they’re replacing. For example think of the energy that’s required to support all of currency traders in the world, the food that they eat, the buildings that they work in, the cars that they drive. Even if the blockchain that replaces them is less efficient than a centralized database, it’s still much more efficient than what we’re using today.\nWhen you start thinking about these types of use-cases for blockchains you see them everywhere. For example consider some of the problems with scientific publishing.\n\n\nExample: scientific publishing\nCurrently scientific work is verified in a somewhat adhoc, distributed manner. People maybe put their work up on a pre-print server, then publish it in a journal, and in some rare cases make their data and code available to the public.\nAfter a work is published, there’s no real way of continuing to verify that it is correct or complete. If a future researcher wants to go back and check whether a paper was retracted, or invalidated, they need to do a literature review of later citations. Similarly if the shared data or code is taken down from where ever it was shared, there’ no real mechanism for recovering it.\nThese are the kinds of problems which are easy to solve with a centralized database. We could imagine some central repository of all published scientific work, and post-publication issues could be noted in that repository. Whenever a researcher wanted to check if the paper had been retracted or called into question they could just look it up on this central repository.\nThis seems like a good solution until you realize that we’ve had databases for 50 years and nobody has gone ahead and created such a repository. This is because there are obstacles to centralization which makes it difficult to do this easy automation work. Some these include:\n\nFinancial incentives do not support centralized publishing\nHaving all scientific knowledge in one database is fragile\nJournals have strong incentives to support the status quo\n\nWith blockchains you could imagine an automation strategy that doesn’t require centralization. Information about papers could be tracked on a blockchain, and when a paper was retracted, that information would be noted in the paper’s record. You could even imagine paying researchers in SciCoin to do peer and post-publication review on papers, or to maintain copies of data and code.\nThe important thing about this it’s possible to get the benefits of automation without needing to first centralize the process. Note that it doesn’t really matter whether this distributed automation is less efficient than centralized automation, because centralized automation is not currently possible.\n\n\nConclusion\nI think this framework is helpful for both blockchain proponents and skeptics. Before declaring that blockchain is the future of something or a giant wast of time, ask whether it’s automating something distributed, or distributing something automated."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Gordon Shotwell",
    "section": "",
    "text": "Hi, my name is Gordon and I’m a data scientist and product manager at Socure.\nMy main professional focus has been helping teams of data scientists build better software. By implementing software development best practices I believe that data scientists can build more reliable and accurate statistical products. I’m a lawyer by training, and remain interested in legal automation and the intersection of law and data science."
  },
  {
    "objectID": "index.html#contact-me",
    "href": "index.html#contact-me",
    "title": "Gordon Shotwell",
    "section": "Contact me",
    "text": "Contact me\nMy favorite networking strategy is to help people out with projects, so feel free to contact me if you think I could be of assistance. I respond to all my emails, and can usually find a half hour to provide advice or assistance.\n\n  \n    \n      How can I help?\n      \n        Data science consultation\n        Mentorship\n        Job opportunity\n        Other\n      \n    \n    \n      Email address\n      \n    \n    \n      Message\n      \n    \n    \n    Submit"
  },
  {
    "objectID": "presentations/secure-systems.html",
    "href": "presentations/secure-systems.html",
    "title": "Building secure systems",
    "section": "",
    "text": "One of the main challenges of doing data science is getting access to the data in the first place. Data scientists need to be able to look at data in detail to do their work effectively, but that necessarily creates data security and data governance problems for the organization and its clients. In this presentation, we go through the social and technical processes that can create a secure data analytics environment and set your team up for success."
  },
  {
    "objectID": "presentations/tech-debt.html",
    "href": "presentations/tech-debt.html",
    "title": "Technical debt is a social problem",
    "section": "",
    "text": "Technical debt is a big problem for the R community. Even though R has excellent support for testing, documentation and packaging code it has the reputation that it is not suitable for production applications because data scientists don’t pay enough attention to technical debt within their codebases. Most people think of technical debt as an engineering problem. We choose to make our current work cheaper at the expense of needing to do more work down the road. But when you look closely at the root causes of technical debt they are almost always about interpersonal relationships. Developers have trouble empathizing with other users of their code and so don’t spend the time to make that code easy for future developers to use and understand. In this talk I argue that we should think about technical debt as a social problem because it gives us insight into why it’s so hard to pay back. I then provide a practical roadmap of how to introduce best practices into your data science team.\nSlides"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "R\n\n\nData Science\n\n\n\n\n\n\n\n\n\n\n\nAug 4, 2022\n\n\nGordon Shotwell\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCovid\n\n\nPersonal\n\n\n\n\n\n\n\n\n\n\n\nJan 11, 2022\n\n\nR package build\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2021\n\n\nGordon Shotwell\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nData Science\n\n\nCovid\n\n\n\n\n\n\n\n\n\n\n\nSep 7, 2021\n\n\nGordon Shotwell\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Science\n\n\nCovid\n\n\nVitamin D\n\n\n\n\n\n\n\n\n\n\n\nNov 21, 2020\n\n\nGordon Shotwell\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nCovid\n\n\nVitamin D\n\n\n\n\n\n\n\n\n\n\n\nOct 23, 2020\n\n\nGordon Shotwell\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVitamin D\n\n\nCovid\n\n\n\n\n\n\n\n\n\n\n\nJun 21, 2020\n\n\nGordon Shotwell\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nCovid\n\n\n\n\n\n\n\n\n\n\n\nMay 23, 2020\n\n\nGordon Shotwell\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVitamin D\n\n\n\n\n\n\n\n\n\n\n\nMay 3, 2020\n\n\nGordon Shotwell\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\nData Science\n\n\n\n\n\n\n\n\n\n\n\nDec 30, 2019\n\n\nGordon Shotwell\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Science\n\n\n\n\n\n\n\n\n\n\n\nApr 19, 2019\n\n\nGordon Shotwell\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Science\n\n\n\n\n\n\n\n\n\n\n\nMay 1, 2018\n\n\nGordon Shotwell\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Science\n\n\nEducation\n\n\n\n\n\n\n\n\n\n\n\nAug 29, 2017\n\n\nGordon Shotwell\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nData Science\n\n\nEducation\n\n\n\n\n\n\n\n\n\n\n\nFeb 2, 2017\n\n\nGordon Shotwell\n\n\n\n\n\n\nNo matching items"
  }
]