---
title: Language models will never be conscious
author: R package build
date: '2022-08-10'
slug: machine-consciousness
draft: true
categories:
  - Data Science
  - philosophy
tags:
  - tag1
  - tag2
keywords:
  - tech
---

Many engineers who work in deep learning are worried about machines gaining consciousness.
Over the last ten years there's been an explosion in tools which use deep learning to solve problems which were previously only the domain of human beings, and this has led many people to believe that it's just a matter of time before these systems gain consciousness.
The worry is that if we're not super careful consciousness will just accidentally emerge from these networks and we will enter a kind of feedback loop where the machine intelligence will take over our society to make itself smarter.
This is a genuine worry for people, it's caused people to quit their jobs, and fund think tanks dedicated to the subject, but I think it's completely misplaced and based on a bad theory of consciousness.

## What do you mean by consciousness?

Most of the people worried about machine consciousness are computer programmers, and as far as I can tell they do not have a very thoughtful articulation of their theory of mind.
Somehow they're profoundly worried about accidentally creating machine consciousness without really articulating what it means for someone or something to be conscious.
I think most of these commentators kind of implicitly believe a theory of mind called the [mind-brain identity theory](https://en.wikipedia.org/wiki/Type_physicalism), which holds that mental states are reducible to a particular organisation of the brian.
Under this theory, what it means to desire or belive something is to have your brain exist in a particular functional state.
In the same way that a physical object *really is* just a bunch of atoms organized in a particular state, your mind *really is* just a bunch of neurons organized in a particular state.

This theory of mind implies that machine consciousness is possible.
After all if all that's required for consciousness is a funtional organization of neurons, then if you replicate those relationships in a computer program you'll have replicated that consciousness.
Unfortunately, the mind-brain identity theory is wrong because it doesn't account for the body.

## A short introduction to cognitive embodiment

Cognitive embodiment is that the body and mind are irreducibly interdependent on one another.
In other words part of what it means to be conscious is to be situated in a particular physical state, and it's nonsensical to describe conscious divorced from that physical state.

Before getting into some of the reasons why this theory of mind is true, it's worth spending a moment to understand what embodiment and preconception.
[Propreoception](https://en.wikipedia.org/wiki/Proprioception) is sense of your body's orientation in space.
If you close your eyes and move your hand you have an awareness of your hand's new position even if you didn't touch anything.
Similarly, you have a pre-conceptual understanding of the physical limits of your body.
You know, for example, that your left leg is a part of "you" in a sense in which your hair or cloths are not.
This awareness is ever-present in our lives, and all of out thoughts and actions take place against this backdrop.
An embodied consciousness is one which exists with that kind of physical awareness.

## Why the body is a precondition for consciousness

Cognitive embodiment is probably the oldest theory of mind, as it's an essential part of the the basis of the [Buddhist account of consciousness](https://en.wikipedia.org/wiki/Skandha), but the clearest argument I've read in modern philosophy is *How the Body Shapes the Mind* by [Shaun Gallagher](academia.edu/865282/Shaun_Gallagher_How_the_Body_Shapes_the_Mind).
I'm going to offer a brief sketch of the argument for embodiment, but if you're interested in this subject I'd really recommend looking at the longer works.

### Phenomenological intuition

The basic intuition of embodiment is that when you look at your own conscious thoughts, or consider other conscious beings you see that they're all irreducibly embodied.
Every thought, feeling, or desire you have exists in relationship to your body, and it's impossible to separate the mental parts of that experience from the physical ones.
For example the experience of riding a roller coaster has mental and physical components, and it's really impossible to figure out where the mental part ends and the physical one begins.
If your heart didn't race in response to your adrenal gland releasing adrenalin it wouldn't be the same experience.
Similarly, if you didn't have the conscious knowledge that the roller coaster was safe, the sensory experience of being on the roller coaster would be different.

Most people are confident that the brain is the organ that generates consciousness, but it's hard to even answer the question of how to separate the brain from the body.
If you think that the electro-chemical signals that make up the brain are consciousness why shouldn't you include the electro-chemical signals in the brain stem?
If you include those why not the rest of the nervous system?
If the nervous system counts as mind why not the limbic system which produces so many psychoactive hormones?
All of these lines seem arbitrary which suggests that the body and mind are a single thing.
We conceptually refer to the 'physical' and 'mental' parts of experience just like we can conceptually differentiate between electricity and magnetism.
In reality they're expressions of a single underlying process.

### Development of higher thinking

The second major argument for cognitive embodiment is that embodied consciousness *precedes* what we think of as higher order thinking.
Children start developing higher order thinking around the age of three or four.
Around this age they start to form long-term memories, express thoughts in complex abstract sentences, and begin to understand numbers.
I'm not sure that the typical four-year-old would be able to pass a Turing test, but they're definitely able to do some relatively difficult tasks.
However they have a self long before they're able to do any of these things.
Very young infants are able to distinguish between self and other, they direct and shape their environment, and as parent will tell you they have clear and obvious desires.
All of these things are possible because the embodied understanding of the self -- who and where you are in the world -- develops much earlier than higher-order cognitive abilities.

The fact that consciousness precedes conceptual thought is one of the main reasons that humans are so much more efficient at learning complex concepts than neural networks.
For example the GPT-3 language model does a pretty good job in understanding text, but building it used a vast supply of energy.
Training the model required billions of training examples, [1.5 gigawatts of electricity](https://blog.scaleway.com/doing-ai-without-breaking-the-bank-yours-or-the-planets/#:~:text=In%20a%20traditional%20data%20center,MWh%20x%201.5%20%3D%201404%20MWh.), and the dedicated work of many scientists.
By contrast the average five year old learns a much wider variety of skills with very few examples, a regular supply of apple slices and buttered noodles, and the attention of a few distracted adults.
Children can learn so efficiently because they have a self *before* they start learning.
This allows them to self-reflect on what they understand and what they don't understand and to challenge their environment to fill in the missing pieces.

The fact that embodied consciousness precedes conceptual ability should at least make you think that embodiment is requirement for consciousness.
Every example of a conscious being you can think of is embodied, and in every case that consciousness started before they were able to work in complex concepts.

## Consciousness is logically dependent on embodiment

The final argument for cognitive embodiment is that consciousness is logically dependent on embodiment.
Embodiment is a logical precondition for consciousness because it defines the limits of the self and it grounds our spatial perception.

Most of our thoughts and beliefs rely in some form on an intuition about the self.
When we say "I'm angry" or "I believe" we're assuming a definition of the self.
Those thoughts really don't make any sense without some kind of axiomatic, intuitive understanding of out own being.
Very few of our thoughts and emotions make sense without this understanding.

Embodiment defines this pre-existing sense of self and so is necessary for consciousness.
Whenever someone expresses a conscious thought they are implicitly referencing their own embodiment.
When I say that the sky is blue, I'm relying on an implicit embodied understanding of the limits of the self.

AI systems do not have a body and so can't devleop this type of self awareness.
For example consider a large language model like [DALLE-2](https://openai.com/blog/dall-e/) which can generate realistic artwork from text imputs.
On the one hand DALLE-2 *looks* conscious because it can do some of the tasks that we associate with consciousness, but since it has no embodied awareness it's hard to understand how a program like DALLE-2 would develop self awareness.
How would DALLE-2 differentiate between self and other?
Is the training data part of its being?
What about the prompts which are fed in by humans?g

Similarly imagine if you replicated the computations of DALLE-2 by asking a billion people to perform the underlying arithmetic using pen and paper.
While this would faithfully replicate the behaviour of the computer program, it's clearly more of an economy or social organization than it is a consciousness.
We're surrounded by complex systems like this which solve incredibly complex problems without developing consciousness.
Economies, ecological systems, and social networks all solve complex problems much better than individual human beings, but they don't have selves because there's nothing to define that self.

At this point someone who worries about conscious machines might say "I'm not worried about machines developing human consciousness, I'm worried about them developing a vast diffuse machine consciousness." I think people who hold this worry have really exited the real of science or philosophy and have started doing a kind of theology.
They look at a complex process and see a god or a demon.
Words like "consciousness", "intelligence", and "self" are all [rigid designators](https://plato.stanford.edu/entries/rigid-designators/) which refer to human-like mental experiences.
Redefining those words to include vague problem-solving organizations is a kind of definitional trick.

# Robots are not embodied

Robots are the main way that so called AI systems can directly interact with the physical world, so it might be natural to think that robots will eventually develop embodiment and the foundation for consciousness.
Robots, however are not and will never be embodied.

There are few strategies for an entity to navigate the physical world.
Humans and all other self-replicating conscious organisms use embodiment.
Our pre-conceptual experience of our body defines both *what* we are and *where* we are in space, and this lets us define our motivations and figure out how to accomplish them physically.
Our actions in physical world are self-contained, we use intrinsic reference points to get around in the world instead of relying on external reference points.
This is a big part of why human beings are such good generalists.
You can pick up a human being and drop them into a totally different environment and they'll do just fine.

Robots on the other hand rely on external reference points to navigate the physical world.
Your Roomba doesn't need an embodied understanding of itself because it has programmers to define its capacities, the base station to ground its perceptual space, and a user to tell it what to do.
The cost of this is that the robot is a terrible generalist, if you give it a novel problem or an environment that's outside of its specification it will tend to fall down.
All of our current robotics projects are designed in this way, because we design robots to accomplish a narrow band of tasks and its easier to do that with extrinsic reference points.

While this is an argument that our *current* robots are not embodied and therefore not conscious, why am I confident that they will *never* gain that capacity?
The reason is economic more than scientific.
The value of robots is that they can displace human workers at specific tasks, and so there's not a lot of value in an embodied generalist robot.

For example, imagine that you're Jeff Bezos and you're being asked to invest in two robotics projects.
One costs a billion dollars and uses established techniques to create a hundred models of specialist robots to replace human warehouse workers while the other costs a hundred billion dollars and relies on speculative technology to develop an embodied generalist robot.
Why would you ever invest in the embodied model when the group of cheap specialists will cost less and perform better?
The only thing you get out of developing an embodied machine is a robot which might have self-awareness and a will of its own, which are bad things from Amazon's perspective.

# Conclusion

The proliferation of increasingly capable statistical systems is and will continue to be profoundly disruptive, but that disruption is more like prior innovations like the railroad or the internet.
It's right to worry about AI because like those other systems it has the potential to entrench systems of power, oppress marginalized groups, and fail catastrophically.
We should not, however, worry about a large language model or other AI system accidentally gaining consciousness.
