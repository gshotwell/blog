{
  "hash": "0b0fdae809fc1d0256692b02eae16922",
  "result": {
    "markdown": "---\ntitle: Testing dbplyr packages\nauthor: Gordon Shotwell\ndate: '2022-08-04'\nslug: dbplyr-testing\ncategories:\n  - R\n  - Data Science\ntags:\n  - tag1\n  - tag2\nkeywords:\n  - tech\n---\n\n\n# SQL generators are good actually\n\nData science students are often told that SQL is the most important tool to learn.\nThis advice makes some sense given how ubiquitous SQL is in industry, but I think it's a bit overrated.\nI've been working as a data scientist for eight or years, and now am a product manager on a large data platform product, but I'm still not entirely sure I really \"know SQL\" in any meaningful way.\nI can sort of read and write simple SQL statements, but I almost always use tools like `dbplyr` instead of writing SQL directly.\nInitially I thought this was a big gap in my knowledge base, and I felt that relying on SQL generators was a kind of crutch, but over time I've become convinced that this is a better way to work.\n\n# \"Good SQL\" is contextual\n\nThe motivation for leaning SQL is that it will allow you to write effective queries.\nThe idea is that if you know SQL very well you'll be able to express your ideas more quickly, and your queries will execute faster.\nThe problem with this is that because different databases differ in how they store data, and in which dialect of SQL they use, there's not really a general way to write effective SQL queries.\nA query which is optimal for a row-oriented database like Redshift may do poorly on a partitioned columnar database like Snowflake.\nMoreover how a specific database is set up will dramatically change query execution speed.\n\nThis was driven home for me recently when I was writing wrapper functions for Socure's new data platform.\nI asked the engineers on the project for advice about how to query the database and they explained that it's not really possible to predict which query will run faster without testing it.\nThe reason for this is that modern databases do a lot of query parsing to optimize the query which is sent by the user into something that can run efficiently on the database.\nThese parsers are quite complex which makes it hard to predict which types of queries will run most efficiently.\n\nConsider the advice to use common table expressions (CTEs) instead of subqueries in a SQL statement.\nThis is usually good advice because it leads to more readable SQL code, but it turns out that it can lead to [600% higher costs](https://medium.com/@AtheonAnalytics/snowflake-query-optimiser-unoptimised-cf0223bdd136) when run against some databases.\nThere's no single set of best practices for writing SQL because what counts as good and bad SQL depends on which specific database you're querying.\n\nAs a result of all of this most data engineers try have more or less abandoned modifying user behaviour.\nInstead of asking the user to send just the right SQL, they modify the database to respond to the queries the user is actually sending.\nFor example if a query is running too slowly, they may set up some kind of view, or change how the data is partitioned to make the query fast.\nThe idea is that the user should write a query which is understandable to them, and the database should take care of\n\n# Writing dbplyr wrappers\n\nA big chunk of my job over the last few years has been writing database wrappers to perform common queries, and whenever possible I try to use dbplyr for these functions.\nIn general I have found that this approach has made it easier for me to write and maintain functions, and the queries that these functions generate are typically as or more performant than writing SQL directly.\nThere are a few main reasons why I prefer this pattern.\n\n### I have to write less code\n\nThe main reason I like using dbplyr functions is that I can leverage the rest of the dbplyr ecosystem to write less code.\nFor example let's take a look at the `nycflights13` data and imagine that we wanted to write a function to get the mean arrival time per airport.\nHere are the dplyr and SQL functions I would write:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"DBI\")\nlibrary(\"dplyr\")\nlibrary(\"duckdb\")\ncon <- dbConnect(duckdb::duckdb(dbdir = \"flights.duckdb\", read_only = TRUE))\n\nmean_dest_time_dbplyr <- function(con = con) {\n  tbl(con, \"flights\") |>\n    group_by(dest) |> \n    summarize(mean_time = mean(arr_time))\n}\n\nmean_dest_time_sql <- function(con = con) {\n  qry <- 'SELECT \"dest\", AVG(\"arr_time\") AS \"mean_time\"\nFROM \"flights\"\nGROUP BY \"dest\"'\n  DBI::dbGetQuery(con, qry)\n}\n```\n:::\n\n\nWhen you start out these functions are about the same, but what happens when you start getting requests from users to add arguments to the function?\nFor example maybe someone wants the function to allow you to filter by air time, if you're patching SQL together you have to do something like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean_dest_time_sql <- function(min_time, max_time, con = con) {\n  where_clause <- paste0('WHERE (\"air_time\" >= ', \n                         min_time, \n                         ') AND (\"air_time\" <= ', \n                         max_time, \n                         ')')\n  \n  qry <- paste0(c('SELECT \"dest\", AVG(\"arr_time\") AS \"mean_time\"',\n                  'FROM \"flights\"',\n                  where_clause,\n                  'GROUP BY \"dest\"'\n                  ),\n                collapse = \"\\n\"\n  )\n  DBI::dbGetQuery(con, qry)\n}\n```\n:::\n\n\nWriting functions like this is annoying because you spend a lot of time pasting queries together, but the bigger issue is that users will never stop asking for functionality.\nAccommodating these requests will lead to a bloated, complex function with a lot of arguments.\nUsing dbplyr relives us of this complication by letting the user to pass any filter they want to our wrapper function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_tbl <- tbl(con, \"flights\") \nmean_dest_time_dbplyr <- function(tbl = my_tbl) {\n  tbl |>\n    group_by(dest) |> \n    summarize(mean_time = mean(arr_time)) \n}\n\n# They can filter by arr_time\nmy_tbl |> \n  filter(arr_time >= 100,\n         arr_time >= 200) |> \n  mean_dest_time_dbplyr()\n\n# But also by other stuff!\n\nmy_tbl |> \n  filter(month == 2) |> \n  mean_dest_time_dbplyr() \n```\n:::\n\n\nThis pattern lets you leverage all of the dbplyr infrastructure which means that you have less code to maintain and less education to do.\n\n### Composable database wrappers \n\nThe second main reason to use dbplyr is that it lets you write composable SQL functions.\nOne of the great things about dbplyr is that it is smart enough to generate adequate SQL regardless of the order in which you call the function.\nFor example putting the filter and mutate in different places will generate different SQL, but both queries will work.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n tbl(con, \"flights\")  |> \n  filter(month == 1) |> \n  mutate(long_flight = ifelse(air_time > 100, \"long\", \"short\")) |> \n  show_query()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<SQL>\nSELECT\n  *,\n  CASE WHEN (\"air_time\" > 100.0) THEN 'long' WHEN NOT (\"air_time\" > 100.0) THEN 'short' END AS \"long_flight\"\nFROM \"flights\"\nWHERE (\"month\" = 1.0)\n```\n:::\n\n```{.r .cell-code}\n tbl(con, \"flights\")  |> \n  mutate(long_flight = ifelse(air_time > 100, \"long\", \"short\")) |> \n  filter(month == 1) |> \n  show_query()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<SQL>\nSELECT *\nFROM (\n  SELECT\n    *,\n    CASE WHEN (\"air_time\" > 100.0) THEN 'long' WHEN NOT (\"air_time\" > 100.0) THEN 'short' END AS \"long_flight\"\n  FROM \"flights\"\n) \"q01\"\nWHERE (\"month\" = 1.0)\n```\n:::\n:::\n\n\nComposable functions are amazing because they let the user build complex expressions out of simple to understand components.\nFor example let's say that we wrote a function `by_day` that grouped the flights data by day.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nby_day <- function(tbl) {\n  tbl |> \n    mutate(date = paste0(year, \"-\", month, \"-\", day)) |> \n    group_by(date)\n}\n```\n:::\n\n\nSince this function can be stacked along with other dbplyr functions it gives the user a lot of flexibility.\nThey can stack it with other dplyr verbs in an arbitrary order, or even use it on an entirely different table, and everything will still work.\n\nSQL is generally speaking not composable.\nYou can't write small fragments of queries and easily insert them into other queries and so your wrapper tends to need to do more work. \nWhen you write wrappers with SQL query construction you end up trying to build a comprehensive function that limits the user to the things that you had in mind when you wrote it.\nAdditionally you can't easily share fragments across functions which means that you end up with repetitive code.\n\n### **Backend-agnostic functions**\n\nFinally, one of the benefits of building dbplyr database wrappers is that your functions will run on a variety of data sources.\nFor example a common pattern at my job is pulling a large set of data into an [Apache Arrow Dataset](https://arrow.apache.org/docs/python/dataset.html) for further analysis.\nFunctions built around dbplyr will tend to work on these datasets without modification which reduces the number of things that the user has to learn or remember.\n\n# Testing dbplyr functions\n\nI've been writing dbplyr wrappers for some time, but I've only recently come up with a testing pattern which I really.\nThere are four main things that I want when testing database functions:\n\n1.  Tests should run without access to the actual database\n\n2.  They should allow me to test the output R object\n\n3.  They should include SQL assertions that I can use to communicate with the database owner\n\n4.  I don't want to regenerate mocks every time I change the function\n\nPreviously I would test database functions with [dittodb](https://dittodb.jonkeane.com/) which allows you to record mocks for particular SQL queries and cache the result of those queries.\nThis accomplished goals 1-3, but over time I found the upkeep difficult.\nBecause dittodb mocks the particular query you end up with a lot of mocks, and you need to regenerate them whenever the function changes.\n\nMy new approach is to record a mock of a few records from the whole database and store that as an on-disk [duckdb database](https://duckdb.org/).\nIn the test files I point my functions to the new database and run two types of tests:\n\n1.  Test that the function produces the right output\n\n2. Test that the function generates the expected SQL\n\nFor example I would test that the `by_day` function produced the right output with a test like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(testthat)\ntest_that(\"by_day function genrates the right output\", {\n  day_counts <- tbl(mock_con, \"flights\") |> \n    by_day() |> \n    count() |> \n    collect(n = 5)\n  \n  expect_s3_class(day_counts, \"data.frame\")\n  expect_equal(dim(day_counts), c(5, 2))\n  expect_equal(day_counts$date,\n               c(\"2013-6-26\", \"2013-6-27\", \"2013-6-28\", \"2013-6-29\", \"2013-6-30\")\n               )\n})\n```\n:::\n\n\nThis gives future developers a clear understanding of what this functions is supposed to do, which lets them make changes with the confidence that they won't violate the user expectations.\nI also want to generate SQL so that I can use it to communicate with the people who maintain the database.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_that(\"by_day function genrates the right SQL\", {\n  expect_snapshot({\n    tbl(mock_con, \"flights\") |> \n      by_day() |> \n      count() |> \n      show_query()\n  })\n})\n```\n:::\n\n\nWhen you run the test suite for the first time this will generate the expected SQL query which is used to test the function in the future:\n\n    Code\n      show_query(count(by_day(tbl(mock_con, \"flights\"))))\n    Output\n      <SQL>\n      SELECT \"date\", COUNT(*) AS \"n\"\n      FROM (\n        SELECT *, CONCAT_WS('', \"year\", '-', \"month\", '-', \"day\") AS \"date\"\n        FROM \"flights\"\n      ) \"q01\"\n      GROUP BY \"date\"\n\nThis is an extremely useful test fixture for two reasons.\nFirst it gives you a something which can be easily shared with the database team.\nFor example if you noticed odd results when running the query against the actual database you can send them the specific query which used to work, but now fails.\nYou can even share this fixture with the database team to use in their tests.\nSecondly, it lets you lock down the expected query.\nThis is useful if you do find out that some types of queries run better on your particular database and want to ensure that future developers don't introduce bad query patterns.\n\n# Conclusion\n\nNo programming framework is comprehensive and there are plenty of times where it's important to move past dbplyr and optimize the actual SQL that your functions generate.\nIn general though I've found that starting with dbplyr saves me time and energy, and produces a better experience for the people who use my functions.\nNine out of ten times dbplyr writes better SQL than I do.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}