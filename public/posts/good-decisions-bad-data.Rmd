---
title: "How to make good decisions from bad data"
author: "Gordon Shotwell"
date: "2020-09-23"
output: html_document
draft: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(patchwork)
library(emojifont)
plot_dist <- function(...) {
  df <- bind_rows(list(...))
  ggplot(df, aes(x = value, group = label, fill = label, color = label)) + 
    theme_minimal() +
    geom_density(alpha = 0.5) +
    scale_y_continuous(labels = NULL, breaks = NULL) +
    ylab("") +
    xlab("Treatment effect") +
    geom_vline(xintercept = 0) +
    scale_x_continuous(limits = c(-4, 4),
                       breaks = c(-2, 0, 2), 
                       labels = c("<- Hurts", "", "Helps ->"))
}

theme_set(theme_light())
```

The dominant view of medical data creates a categorical distinction between randomized clinical trial data and other forms of data. 
Under this view the only information that can ground medical decision making is a large, multicenter, randomized clinical trial, and all other forms of information are just correlation. 
People who hold this view treat clinical trials as determinative of casaution. 
Without a clinical trial you can't make a causal claim, and once you have one you no longer need to think that hard about casaution. 


### What do we get from a clinical trial?

The purpose of clinical trials is to estimate the distribution of an outcome.
You design and conduct an experiment, and based on the results and some statistical assumptions you can estimate the range of outcomesthat would see in the world. 
All the things we use to talk about clinical results like confidence intervals, p-values, or statistical significance are just ways of reporting this distribution, but we can conceptualize it with a plot like this one.

```{r, echo = FALSE}
works <- tibble(
  value = rnorm(10000, 2, 0.2), 
  label = "Drug works!"
)

dont_work <- tibble(
  value = rnorm(10000, 0, 0.5), 
  label = "We don't know!"
)

hurts_people <- tibble(
  value = rnorm(10000, -2, 0.3), 
  label = "Drug hurts people!"
)


plot_dist(works, dont_work, hurts_people)
```

These distributions have a mean, which is usually the estimated treatment effect, and standard deviation which tells you something about how certain you should be in the results.
The reported distribution is, however, only half the story because it only accounts for quantifiable error.

All experimental data has two types of error, quantifiable error and unquantifiable, or irreducible error. 
Quantifiable error is basically the known unknowns of experimental data. 
For any dataset you have some estimate of how likely that data was to arise due to chance based on the variance of that data, and assumptions about the true distribution in the world. 

However quantifiable error is not the only, or most important, form of error. 

There are often problems with trials which relate with the way the trial was designed, structured, or executed. 
For example there could be a systematic, unknown error with a particular test, or Excel could have turned a bunch of genetic signatures into dates, or maybe the trial was just entirely fraudulent. 
All of these things are sources of uncertainty, and importantly, we cannot estimate them from a single dataset because estimates will never include the risk that the data itself is flawed.
The people who undertake the study will do their best to prevent these problems, but the risk of them is always there. 

The result is that for any study we really have two distributions of effects, the reported distribution which includes just the quantifiable error, and the true distribution which includes the unquantifiable error as well. 

```{r, echo = FALSE}
real <- tibble(
  value = rnorm(10000, 2, 0.3), 
  label = "True"
)

reported <- tibble(
  value = rnorm(10000, 2, 0.2), 
  label = "Reported"
)

plot_dist(real, reported)
```

What makes clinical trials special is that they are able to isolate a single mechanism from a broad range of confounders, and so the reported distribution is usually pretty close to the true one. 
Observational or ecological data can't do this effectively so the true distribution is usually much wider than the reported one. 
An observational study might report a highly significant result, but because there are lots of potential causal mechanisms which are not controlled for it's a poor estimate of what's actually going on in the world. 
The result of these confounders is the numbers reported by the study will usually be overly precise.
The true distribution of effects is much larger than that reported by the study.

```{r, echo = FALSE}
df2 <- tibble(
  value = rnorm(10000, 1.3, 1.2), 
  label = "True"
)

plot_dist(df2, reported)
```

The important thing is that all evidence whether it be experimental, observational, or ecological has this problem. 
What generally makes randomized trials "good" and ecological trials "bad" is that for clinical trials the real distribution will be close to the reported distribution, while for ecological study it will be much wider. 
The important thing is that this is not a categorical distinciton,
there are well designed observational studies that provide an accurate representation of the world, and poorly executed clinical trials which do not.
All these trials  have flaws, so you shouldn't just believe any of them, and they all provide information, so you shouldn't ignore them either.

Given that every trial is an imperfect picture of the world, how can make decisions based on them?

# Replicate, triangulate

One of the most important things to look for when evaluating a trial is whether it's been replicated. 
Replication makes it much more likely that a trial is a good picture of the world because most causes of irreducible error are eliminated if you have two different researchers do the trial. 
For example, it's possible that one researcher fabricated data, but it's vanishingly unlikely that several of them did so in the exact same way. 
As a result if you have two, relatively independent, trials replicate the same results it is more likely that those results are real and not due to mistake of malfeasance. 

Replication cannot, however, reveal systematic issues with the trial design, because those issues will be propegated to all the places where it was replicated. 
I'm going to illustrate this with my favourite example of whether vitamin D deficiency causes severe Covid-19 complications.

The first papers to suggest that vitamin D caused severe Covid were kind of fishy observational papers. 
The study design was to look at hospitalized patients and ask whether vitamin D levels were associated with Covid 19 progression, and they found that they were. 
Since then this result has [been replicated](https://vitamin-d-covid.shotwell.ca/#post-infection-blood-samples) in 18 other papers, 
this is great but it doesn't help our decision that much because these papers all share the same problems.
Since we're observing vitamin D levels after infection we don't know what caused what, it's possible that vitamin D deficiency causes Covid, it's possible that Covid causes vitamin D deficiency, and it's possible that some unknown factor cause both of them. 

To put it visually the data is equally consistent with the following three causal diagrams, and so replicating it doesn't give us much more certainty. 

```{r, echo = FALSE}
library(DiagrammeR)


grViz("
digraph with_title {
  graph [overlap = true, 
  fontsize = 20,
  label = 'Causal\n\n',
  labelloc = 't',
  rankdir = LR]
  # several 'node' statements
  node [shape = box,
  fontsize = 12,
        fontname = Helvetica]
  D [label = 'Vitamin D deficiency']
  C [label = 'Severe Covid']; 
  D -> C
}
")

grViz("
digraph with_title {
  graph [overlap = true, 
  fontsize = 20,
  label = 'Reverse Causal\n\n',
  labelloc = 't',
  rankdir = LR]
  # several 'node' statements
  node [shape = box,
  fontsize = 12,
        fontname = Helvetica]
  D [label = 'Vitamin D deficiency']
  C [label = 'Severe Covid']; 
  C -> D;
}
")

grViz("
digraph with_title {
  graph [overlap = true, 
  fontsize = 20,
  label = 'Confounder\n\n',
  labelloc = 't',
  rankdir = LR]
  # several 'node' statements
  node [shape = box,
  fontsize = 12,
        fontname = Helvetica]
  D [label = 'Vitamin D deficiency']
  C [label = 'Severe Covid']; 
  U [label = 'Unknown risk factor']
  U -> D 
  U -> C
}
")
```

Triangulation is basically the process of looking at a diverse set of study designs and asking what causal theory is consistent with all of them.
The statistical insight here is the same as that underlying ensemble models and polling averages:
if you have a bunch of different types of evidence, their signal will be correlated but the error will be uncorrelated so if you take a global perspective on the data, the error and bias of the individual trials will tend to cancel each-other out. 

In the vitamin D example we have [four observational studies](https://vitamin-d-covid.shotwell.ca/#pre-infection-blood-samples) which found an association between _pre-infection_ blood samples and Covid-19 which makes the reverse causal model less likely. 
Since vitamin D levels which were measured before a person became sick are also correlated with Covid-19, it's less likely that Covid was causing vitamin D deficiency. 

We also have [six studies](https://vitamin-d-covid.shotwell.ca/#supplementation-studies), including two randomized clinical trials which found that vitamin D _supplementation_ improves disease progression in Covid positive patients.
The two trials found that supplementation [reduced](https://www.sciencedirect.com/science/article/pii/S0960076020302764?via%3Dihub#tbl0005) ICU visits, and [increased the proportion](https://pmj.bmj.com/content/early/2020/11/12/postgradmedj-2020-139065.full) of mildly symptomatic people who were able to clear the virus.

Neither of the randomized trials are definitive, but they are enough to rule out the confounder model. The confounder model is that idea the association between vitamin D deficiency in the observational studies was due to some unknown confoudner that was causing both vitamin D deficiency and Covid-19 severity.
This may be something like spending more time outside or having a condition which both reduces your vitamin D levels and also increase your Covid jeopardy but which was not controlled for in the observational data. 
Randomization, even with small sample sizes, will control for these unknown confounding factors because they will randomly be distributed between the treatment and control arms. 
Additionally, the confounder model is inconsistent with the idea that Covid risk can be treated with vitamin D supplementation, if that model were true than these trials should either show no effect, or at least a fairly small effect rather than the large effect sizes that they did in fact report. 

This approach also lets you incorporate evidence from contradictory trials without necessarily needing to pick one. 
For example just last week there was a 240 person randomized controlled trial that found that a single high dose vitamin D supplement given 10 days after symptom onset did not improve the outcomes of severely ill Covid patients. 
If you were evaluating this using heuristics you might just look at the sample size of this trial and declare it the winner because it's larger than the other two trials. 
In other words you pick the strongest piece of evidence, believe it entirely, and discard any other pieces of evidence. 
A better approach is to at least try to incorporate this evidence together and see if there's a plausible causal story that fits all the data points. 
It's unlikely that all but one of the studies are totally wrong, so if there's a causal story which incorporates all of the evidence that's more appealing than a winner-take-all approach where the best study is believed completely while the others are discarded. 
In this case one way of reconciling the large failed RCT with the small, successful ones is that vitamin D is not effective in severely ill patients during Covid's [inflammatory phase](https://twitter.com/DanielGriffinMD/status/1321867994029580288) but is effective during the pre-infection, and viral replication phase. 

# Think about expected value

The second way to make good decisions based on bad data is to consider the expected value of a decision. 
Remember what makes bad studies bad is that they don't do a good job of estimating the real world value of an intervention, and so they can't provide a precise estimate of what's going to happen in the world. 
However, for very low cost interventions we often don't need a precise estimate to make a good treatment decision. 

Consider the idea of wearing a cloth mask to prevent Covid-19 infection.
The evidence in favour of this intervention is really bad. 
We have a few mechanistic studies which show that, in labratory conditions, a cloth mask can reduce the transmission of viral particles, and we have a bunch of highly confounded ecological studies that show that places that have a mask mandate (as well as many other things) have a reduced Covid disease burden. 
There is a recent randomized study of mask usage, which failed to show that giving people masks reduced their risk of contracting Covid-19. 
If we go back to our distribution plots, the mask evidence looks a lot like this, there's some reported benefit but the available data really can't provide that precise an estimate that masks work. 

```{r, echo = FALSE}

df <- tibble(
  value = rnorm(10000, mean = 2, sd = 0.3), 
  label = "Reported"
)

df2 <- tibble(
  value = runif(10000, min = -2, max = 4), 
  label = "True"
)

plot_dist(df, df2)
```

When you incorporate other information about the costs and benefits of mask usage, however, it becomes clear that the only good decision is to recommend masks. 
In particular we have very good evidence that wearing a mask isn't really harmful. 
Doctors, nurses, and dentists all wear masks all day, and do not suffer ill effects. 
Mask wearing is common in many Asian countries and there aren't any reports of ill effects. 
While we're not really sure about the upside of mask wearing, we can be very sure that they don't have any downside. 
Incorporating this information leads to a graph like this: 

```{r, echo = FALSE}
df2 <- tibble(
  value = runif(10000, min = 0, max = 4), 
  label = "True"
)

plot_dist(df, df2)
```

The important thing about this graph is that it allows us to make a decision based on the evidence *even if we think that evidence is terrible*. 
While  we can't estimate precisely how much masks help, we can be pretty sure that they aren't going to hurt us. 
The upside is some positive number, the downside is pretty much zero, and so the expected value of mask wearing will be positive. 
From the data we can't tell *how* positive, but we really don't need to know that,
since Covid is extremely serious we should be taking every action that has a positive expected value, even if that value is pretty small. 
So the poor quality ecological data and mechanistic studies can reveal that it's plausible that masks reduce Covid transmission, and that plausibility is all we should need to adopt the intervention. 

# Conclusion

We have no choice but to make a decision on whether to adopt a particular habit or intervention, 
and we have to make those decisions based on the data that's in front of us. 
We're lucky if we're able to make those decisions based on several strong clinical trials, 
but more often than not the only information we have is based on small, biased, confounded studies. 
However, if we think through the totalilyt of the evidence, as well as the expected value of our actions, we can usually make a better decision than just waiting for more evidence. 



































