---
title: "Testing machine learning models with testthat"
date: "2018-05-01"
output: html_document
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>Automated testing is a huge part of software development. Once a project reaches a certain level of complexity, the only way that it can be maintained is if it has a set of tests that identify the main functionality and allow you to verify that functionality is intact. Without tests, it’s difficult or impossible to identify where errors are occurring, and to fix those errors without causing further problems.</p>
<ol style="list-style-type: decimal">
<li>Tests codify your expectations at the point when you actually remember what you’re expecting</li>
<li>They allow you to offload verification to a machine</li>
<li>You can make changes to your code with confidence</li>
</ol>
<p>Data science projects tend to be pretty under-tested, which is unfortunate because they have all of the same complexity and maintainability issues as software projects. For instance, if the packages you use change, the data changes, or you just go back and try to make some changes yourself you run the risk of breaking your analysis in ways thatcan be very difficult to detect. There’s been a fair amount of excellent writing on <a href="http://r-pkgs.had.co.nz/tests.html">testing functionality</a> and <a href="https://github.com/ropensci/testdat">testing data</a> but one of the best types of data science testing is to test the statistical model itself.</p>
<p>Statistical models are often developed under rigorous observation but then deployed into a production system where they might not be monitored as closely. During the development process a data scientist will hopefully spend a lot of time interrogating the model to make sure that it’s not over-fitting, making use of bad data, or misbehaving in some other manner - but once the model is deployed they probably move on to other things and might not worry too much about whether the model is continuing to function as they expected. This leaves the model open to two big classes of bugs:</p>
<ol style="list-style-type: decimal">
<li><p>The model’s inputs change
This might happen if the data collection process changes, the modelling function itself changes, or you change something about how the data is processed before it’s fed into the model. Sometimes this will result in dramatic failures that are easy to identify. For instance, your model might start producing noticeably bad predictions which users complain about, but it might also produce subtle problems that you can’t easily detect. The model might just produce worse predictions, or do so only for a subset of users.</p></li>
<li><p>The model is refit on invalid data
One of the great features of modern machine learning techniques is that they can continue to learn from new data. Models can be fit on a historical dataset, but then updated on live data. This allows the model to make use of the largest set of information, but leaves it open to bias and over-fitting if the new data has problems that you couldn’t detect in the historical data. An extreme example might be if a bug in the data collection process resulted in a single user’s record being repeated multiple times. If the model is refit on that data, it would begin to fit to the peculiarities of that user and perform poorly on other records.</p></li>
</ol>
<div id="what-do-tests-offer" class="section level2">
<h2>What do tests offer?</h2>
<p>The main benefit of testing software is that it helps clarify what exactly you expect from your code. Whenever we write a line of code we have some fuzzy sense of what that code should do but that sense isn’t super precise. We might look at the result of a function and say “that looks a bit fishy” or “that looks realistic” without stopping to clarify <em>why</em> it looks fishy or <em>why</em> it’s realistic. When you go to write a test you have to be more specific, which helps to make your thought process more precise. For instance, we might know that a person’s height can’t be too small, but in order to write a test you need to specify what “too small” means.</p>
<p>The same is true for modelling. There are lots of ways to get a sense of whether your model is fishy or not during the model development process. You might check how the model performs on new data, or check colinearity of its features. You could check whether the model performs well on easy cases, or make sure that the predictions aren’t too good to be true. The key point about testing models is that you should clarify and write down your expectations of the model. How <em>should</em> it perform on these different metrics? When should you start worrying about it?</p>
</div>
<div id="questions-to-ask-about-your-model" class="section level2">
<h2>Questions to ask about your model</h2>
<p>The answers to these questions are domain- and technique-specific. An analyst will ask very different questions depending on what they are modelling, the particular techniques they are using, and their background. Here are a few examples of tests, to hopefully get you started testing statistical models.</p>
<p>Let’s start with a simple model of house prices from <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques">Kaggle</a>.</p>
<pre class="r"><code>library(tidyverse)
library(broom)
library(testthat)
house &lt;- read_csv(&quot;house_prices.csv&quot;)
model &lt;- lm(SalePrice ~ LotArea + Neighborhood, house)
house$pred &lt;- predict(model)
rmse = function(predicted, observed){
  sqrt(mean((predicted - observed)^2))
}</code></pre>
<div id="is-it-performing-well-enough" class="section level4">
<h4>Is it performing well enough?</h4>
<p>The most basic question you can ask about a model is whether it’s continuing to perform well as new data comes in. Typically when you develop a model for production you have made some kind of promise to your company or customer about how that model performs. For instance, it might be worth predicting a customer’s next purchase if the model is 50% accurate, but not worth doing so if it is only 2% accurate. A model can perform well on the training data, but if customer behavior changes in some way its accuracy can degrade. Instead of being notified of these changes by angry product managers, you can write a test that tells you when the model accuracy falls below a certain threshold. In this case we have a root mean squared error of 5.1891341^{4} dollars so let’s say we want to worry about the model when that error gets bigger than $75,000. I’m going to generate new data by sampling my existing dataset, but in production you should use actual new data. The test would look something like this:</p>
<pre class="r"><code>library(testthat)
new_data &lt;- house[sample(1:nrow(house), replace = TRUE), ]
test_that(&quot;model rmseis above threshold&quot;, {
  new_data$pred &lt;- predict(model, newdata = new_data)
  expect_true(rmse( new_data$pred, new_data$SalePrice) &lt; 75000)
})</code></pre>
<p>It’s often helpful to check the other side of model performance as well to ensure that the model isn’t performing unnaturally well. This can happen in real life systems as users learn to game machine learning models; for example, if you have a model of student performance that rewards teachers based on how their students are doing, over time student and teacher behavior might start to shift to accord with the model. A good place to start with these kinds of tests is the intuition that your model performance should probably be worse in the wild than it was when you first developed it. If it starts improving you probably want to do some investigation to understand why.</p>
<pre class="r"><code>test_that(&quot;model rmse isn&#39;t too hight&quot;, {
  expect_true(rmse( new_data$pred, new_data$SalePrice) &gt; 20000)
})</code></pre>
<p>This approach is flexible in that you can test all of the model performance metrics which you used to develop your model. In fact you can write these tests out <em>before</em> you develop the model and use them to test the model as you develop it. This can be a helpful way to prevent yourself from cherry picking measures of performance that look good for your particular data.</p>
</div>
<div id="are-the-predictions-within-a-sensible-range" class="section level4">
<h4>Are the predictions within a sensible range?</h4>
<p>A common form of sanity check for a model is to generate some predictions and check that they are within a sensible range. For instance we might want to check that we’re not predicting negative house prices, or house prices which are extremely large.</p>
<pre class="r"><code>test_that(&quot;model predictions are sensible&quot;, {
  expect_true(min(new_data$pred) &gt; 0)
  expect_true(max(new_data$pred) &lt; 2 * max(house$SalePrice))
})</code></pre>
<p>Depending on your domain you might also want to check if the model is predicting outcomes that weren’t really part of the training set. For instance our model didn’t see very small or very large house prices so we might have some issues with those predictions. These tests check that most of the model’s predictions are within the range of our training data.</p>
<pre class="r"><code>test_that(&quot;model predictions are within training range&quot;, {
  small_predictions &lt;- mean(new_data$pred &lt; min(house$SalePrice)) 
  large_predictions &lt;- mean(new_data$pred &gt; min(house$SalePrice))
  expect_true(small_predictions &lt; 0.01)
  expect_true(large_predictions &lt; 0.01)
})</code></pre>
</div>
<div id="how-does-the-refit-model-look" class="section level4">
<h4>How does the refit model look?</h4>
<p>Refitting models automatically in production is a great way of continuing to learn from new data, but you lose the human oversight of the model which can lead to over-fitting. In addition to the tests listed above you can do some basic checks on the model itself to make sure that it’s being generated properly.</p>
<pre class="r"><code>library(broom)
new_model &lt;- lm(SalePrice ~ LotArea + Neighborhood, new_data)

#Did this actually generate a model?
expect_is(new_model, &quot;lm&quot;)
expect_true(glance(new_model)$r.squared &gt; 0.5)

# Check outliers among the Neighborhood dummy variables
max_neighborhood_beta &lt;- new_model %&gt;% 
  tidy() %&gt;%  
  filter(str_detect(term, &quot;Neighborhood&quot;)) %&gt;% 
  pull(estimate) %&gt;% 
  max(abs(.))
expect_true(max_neighborhood_beta &lt; 2e5)</code></pre>
<p>Again you can substitute whatever you want for measures of model performance. The important thing is that you clarify what you are expecting and have your computer check that those expectations are met every time the model is re-fit.</p>
</div>
<div id="is-it-making-illegal-or-unethical-decisions" class="section level4">
<h4>Is it making illegal or unethical decisions?</h4>
<p>Testing is a great way to check whether your model is making distinctions that it shouldn’t. I’m a lawyer by training so I’m aware of some of the problems companies can run into when they start making illegal distinctions between customers. For instance, if you implement a model that sentences people differently based on their race or gender, your company runs the risk of legal sanction. These sanctions can be ruinous for a company, so it’s definitely worth writing a test about.</p>
<p>A good way of evaluating whether your model is making these distinctions is to sample from various ethnic or gender groups in your data, run those samples through your data pipeline, and check that the predictions are within the same range. If they are not the same, it’s worth worrying about whether your model is discriminatory. In Canada there’s something called “adverse effect discrimination,” which means you can be liable for a model that discriminates against a particular group even if you don’t include information about group membership in your model. If you consult with a lawyer about what kind of distinctions are allowed, you can then build tests to ensure that you and future data scientists are not stepping over those lines.</p>
<p>To show how this test would work, let’s generate racial categories for this data.</p>
<pre class="r"><code>house$seller_race &lt;- sample(c(&quot;Aboriginal&quot;, &quot;Black&quot;, &quot;White&quot;), nrow(house), TRUE)

sub_groups &lt;- house %&gt;% 
  group_by(seller_race) %&gt;% 
  nest() %&gt;% 
  mutate(preds = map(data, ~predict(model, .))) %&gt;% 
  mutate(mean_predicted = round(map_dbl(preds, mean))) %&gt;% 
  select(seller_race, mean_predicted) %&gt;% 
  spread(seller_race, mean_predicted)

test_that(&quot;No significant difference between racial groups&quot;, {
  expect_false(sub_groups$Aboriginal[1] &lt; sub_groups$White[1])
  expect_false(sub_groups$Black[1] &lt; sub_groups$White[1])
})</code></pre>
<p>My guess is that a large proportion of data science models would fail checks like this because so many datasets on which those models are based are biased in one way other another. The result is that even if the model doesn’t explicitly rely on racial or gender by including those features in the model, they might be systematically biased against minotiry groups. This should make everybody worried as these models get embedded into more aspects of our lives. For instance a model which generates home loan interest rates might charge minority groups higher rates, or a sentencing model might prevent minority groups from qualifying for bail. These models are likely illegal, and certainly unethical so it’s worth creating tests which check that you are not accidentally making those kinds of distinctions.</p>
</div>
</div>
